{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAt-K2qgcIou"
   },
   "source": [
    "# Neural Network with Two Layers\n",
    "\n",
    "Welcome to your week three programming assignment. You are ready to build a neural network with two layers and train it to solve a classification problem. \n",
    "\n",
    "**After this assignment, you will be able to:**\n",
    "\n",
    "- Implement a neural network with two layers to a classification problem\n",
    "- Implement forward propagation using matrix multiplication\n",
    "- Perform backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Classification Problem](#1)\n",
    "- [ 2 - Neural Network Model with Two Layers](#2)\n",
    "  - [ 2.1 - Neural Network Model with Two Layers for a Single Training Example](#2.1)\n",
    "  - [ 2.2 - Neural Network Model with Two Layers for Multiple Training Examples](#2.2)\n",
    "  - [ 2.3 - Cost Function and Training](#2.3)\n",
    "  - [ 2.4 - Dataset](#2.4)\n",
    "  - [ 2.5 - Define Activation Function](#2.5)\n",
    "    - [ Exercise 1](#ex01)\n",
    "- [ 3 - Implementation of the Neural Network Model with Two Layers](#3)\n",
    "  - [ 3.1 - Defining the Neural Network Structure](#3.1)\n",
    "    - [ Exercise 2](#ex02)\n",
    "  - [ 3.2 - Initialize the Model's Parameters](#3.2)\n",
    "    - [ Exercise 3](#ex03)\n",
    "  - [ 3.3 - The Loop](#3.3)\n",
    "    - [ Exercise 4](#ex04)\n",
    "    - [ Exercise 5](#ex05)\n",
    "    - [ Exercise 6](#ex06)\n",
    "  - [ 3.4 - Integrate parts 3.1, 3.2 and 3.3 in nn_model()](#3.4)\n",
    "    - [ Exercise 7](#ex07)\n",
    "    - [ Exercise 8](#ex08)\n",
    "- [ 4 - Optional: Other Dataset](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "First, import all the packages you will need during this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "# A function to create a dataset.\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Output of plotting commands is displayed inline within the Jupyter notebook.\n",
    "%matplotlib inline \n",
    "\n",
    "# Set a seed so that the results are consistent.\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the unit tests defined for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import w3_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one of the labs this week, you trained a neural network with a single perceptron, performing forward and backward propagation. That simple structure was enough to solve a \"linear\" classification problem - finding a straight line in a plane that would serve as a decision boundary to separate two classes.\n",
    "\n",
    "Imagine that now you have a more complicated problem: you still have two classes, but one line will not be enough to separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.4\n",
    "x_line = np.arange(xmin, xmax, 0.1)\n",
    "# Data points (observations) from two classes.\n",
    "ax.scatter(0, 0, color=\"r\")\n",
    "ax.scatter(0, 1, color=\"b\")\n",
    "ax.scatter(1, 0, color=\"b\")\n",
    "ax.scatter(1, 1, color=\"r\")\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "# Example of the lines which can be used as a decision boundary to separate two classes.\n",
    "ax.plot(x_line, -1 * x_line + 1.5, color=\"black\")\n",
    "ax.plot(x_line, -1 * x_line + 0.5, color=\"black\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logic can appear in many applications. For example, if you train a model to predict whether you should buy a house knowing its size and the year it was built. A big new house will not be affordable, while a small old house will not be worth buying. So, you might be interested in either a big old house, or a small new house.\n",
    "\n",
    "The one perceptron neural network is not enough to solve such classification problem. Let's look at how you can adjust that model to find the solution.\n",
    "\n",
    "In the plot above, two lines can serve as a decision boundary. Your intuition might tell you that you should also increase the number of perceptrons. And that is absolutely right! You need to feed your data points (coordinates $x_1$, $x_2$) into two nodes separately and then unify them somehow with another one to make a decision. \n",
    "\n",
    "Now let's figure out the details, build and train your first multi-layer neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Neural Network Model with Two Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 - Neural Network Model with Two Layers for a Single Training Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nn_model_2_layers.png\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output layers of the neural network are the same as for one perceptron model, but there is a **hidden layer** now in between them. The training examples $x^{(i)}=\\begin{bmatrix}x_1^{(i)} \\\\ x_2^{(i)}\\end{bmatrix}$ from the input layer of size $n_x = 2$ are first fed into the hidden layer of size $n_h = 2$. They are simultaneously fed into the first perceptron with weights $W_1^{[1]}=\\begin{bmatrix}w_{1,1}^{[1]} & w_{2,1}^{[1]}\\end{bmatrix}$, bias  $b_1^{[1]}$; and into the second perceptron with weights $W_2^{[1]}=\\begin{bmatrix}w_{1,2}^{[1]} & w_{2,2}^{[1]}\\end{bmatrix}$, bias $b_2^{[1]}$. The integer in the square brackets $^{[1]}$ denotes the layer number, because there are two layers now with their own parameters and outputs, which need to be distinguished. \n",
    "\n",
    "\\begin{align}\n",
    "z_1^{[1](i)} &= w_{1,1}^{[1]} x_1^{(i)} + w_{2,1}^{[1]} x_2^{(i)} + b_1^{[1]} = W_1^{[1]}x^{(i)} + b_1^{[1]},\\\\\n",
    "z_2^{[1](i)} &= w_{1,2}^{[1]} x_1^{(i)} + w_{2,2}^{[1]} x_2^{(i)} + b_2^{[1]} = W_2^{[1]}x^{(i)} + b_2^{[1]}.\\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "These expressions for one training example $x^{(i)}$ can be rewritten in a matrix form :\n",
    "\n",
    "$$z^{[1](i)} = W^{[1]} x^{(i)} + b^{[1]},\\tag{2}$$\n",
    "\n",
    "where \n",
    "\n",
    "&emsp; &emsp; $z^{[1](i)} = \\begin{bmatrix}z_1^{[1](i)} \\\\ z_2^{[1](i)}\\end{bmatrix}$ is vector of size $\\left(n_h \\times 1\\right) = \\left(2 \\times 1\\right)$; \n",
    "\n",
    "&emsp; &emsp; $W^{[1]} = \\begin{bmatrix}W_1^{[1]} \\\\ W_2^{[1]}\\end{bmatrix} = \n",
    "\\begin{bmatrix}w_{1,1}^{[1]} & w_{2,1}^{[1]} \\\\ w_{1,2}^{[1]} & w_{2,2}^{[1]}\\end{bmatrix}$ is matrix of size $\\left(n_h \\times n_x\\right) = \\left(2 \\times 2\\right)$;\n",
    "\n",
    "&emsp; &emsp; $b^{[1]} = \\begin{bmatrix}b_1^{[1]} \\\\ b_2^{[1]}\\end{bmatrix}$ is vector of size $\\left(n_h \\times 1\\right) = \\left(2 \\times 1\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the hidden layer activation function needs to be applied for each of the elements in the vector $z^{[1](i)}$. Various activation functions can be used here and in this model you will take the sigmoid function $\\sigma\\left(x\\right) = \\frac{1}{1 + e^{-x}}$. Remember that its derivative is $\\frac{d\\sigma}{dx} = \\sigma\\left(x\\right)\\left(1-\\sigma\\left(x\\right)\\right)$. The output of the hidden layer is a vector of size $\\left(n_h \\times 1\\right) = \\left(2 \\times 1\\right)$:\n",
    "\n",
    "$$a^{[1](i)} = \\sigma\\left(z^{[1](i)}\\right) = \n",
    "\\begin{bmatrix}\\sigma\\left(z_1^{[1](i)}\\right) \\\\ \\sigma\\left(z_2^{[1](i)}\\right)\\end{bmatrix}.\\tag{3}$$\n",
    "\n",
    "Then the hidden layer output gets fed into the output layer of size $n_y = 1$. This was covered in the previous lab, the only difference are: $a^{[1](i)}$ is taken instead of $x^{(i)}$ and layer notation $^{[2]}$ appears to identify all parameters and outputs:\n",
    "\n",
    "$$z^{[2](i)} = w_1^{[2]} a_1^{[1](i)} + w_2^{[2]} a_2^{[1](i)} + b^{[2]}= W^{[2]} a^{[1](i)} + b^{[2]},\\tag{4}$$\n",
    "\n",
    "&emsp; &emsp; $z^{[2](i)}$ and $b^{[2]}$ are scalars for this model, as $\\left(n_y \\times 1\\right) = \\left(1 \\times 1\\right)$; \n",
    "\n",
    "&emsp; &emsp; $W^{[2]} = \\begin{bmatrix}w_1^{[2]} & w_2^{[2]}\\end{bmatrix}$ is vector of size $\\left(n_y \\times n_h\\right) = \\left(1 \\times 2\\right)$.\n",
    "\n",
    "Finally, the same sigmoid function is used as the output layer activation function:\n",
    "\n",
    "$$a^{[2](i)} = \\sigma\\left(z^{[2](i)}\\right).\\tag{5}$$\n",
    "\n",
    "Mathematically the two layer neural network model for each training example $x^{(i)}$ can be written with the expressions $(2) - (5)$. Let's rewrite them next to each other for convenience:\n",
    "\n",
    "\\begin{align}\n",
    "z^{[1](i)} &= W^{[1]} x^{(i)} + b^{[1]},\\\\\n",
    "a^{[1](i)} &= \\sigma\\left(z^{[1](i)}\\right),\\\\\n",
    "z^{[2](i)} &= W^{[2]} a^{[1](i)} + b^{[2]},\\\\\n",
    "a^{[2](i)} &= \\sigma\\left(z^{[2](i)}\\right).\\\\\n",
    "\\tag{6}\n",
    "\\end{align}\n",
    "\n",
    "Note, that all of the parameters to be trained in the model are without $^{(i)}$ index - they are independent on the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the predictions for some example $x^{(i)}$ can be made taking the output $a^{[2](i)}$ and calculating $\\hat{y}$ as: $\\hat{y} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5, \\\\ 0 & \\mbox{otherwise }. \\end{cases}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Neural Network Model with Two Layers for Multiple Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the single perceptron model, $m$ training examples can be organised in a matrix $X$ of a shape ($2 \\times m$), putting $x^{(i)}$ into columns. Then the model $(6)$ can be rewritten in terms of matrix multiplications:\n",
    "\n",
    "\\begin{align}\n",
    "Z^{[1]} &= W^{[1]} X + b^{[1]},\\\\\n",
    "A^{[1]} &= \\sigma\\left(Z^{[1]}\\right),\\\\\n",
    "Z^{[2]} &= W^{[2]} A^{[1]} + b^{[2]},\\\\\n",
    "A^{[2]} &= \\sigma\\left(Z^{[2]}\\right),\\\\\n",
    "\\tag{7}\n",
    "\\end{align}\n",
    "\n",
    "where $b^{[1]}$ is broadcasted to the matrix of size $\\left(n_h \\times m\\right) = \\left(2 \\times m\\right)$ and $b^{[2]}$ to the vector of size $\\left(n_y \\times m\\right) = \\left(1 \\times m\\right)$. It would be a good exercise for you to have a look at the expressions $(7)$ and check that sizes of the matrices will actually match to perform required multiplications.\n",
    "\n",
    "You have derived expressions to perform forward propagation. Time to evaluate your model and train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - Cost Function and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation of this simple neural network you can use the same cost function as for the single perceptron case - log loss function. Originally initialized weights were just some random values, now you need to perform training of the model: find such set of parameters $W^{[1]}$, $b^{[1]}$, $W^{[2]}$, $b^{[2]}$, that will minimize the cost function.\n",
    "\n",
    "Like in the previous example of a single perceptron neural network, the cost function can be written as:\n",
    "\n",
    "$$\\mathcal{L}\\left(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\right) = \\frac{1}{m}\\sum_{i=1}^{m} L\\left(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\right) =  \\frac{1}{m}\\sum_{i=1}^{m}  \\large\\left(\\small - y^{(i)}\\log\\left(a^{[2](i)}\\right) - (1-y^{(i)})\\log\\left(1- a^{[2](i)}\\right)  \\large  \\right), \\small\\tag{8}$$\n",
    "\n",
    "where $y^{(i)} \\in \\{0,1\\}$ are the original labels and $a^{[2](i)}$ are the continuous output values of the forward propagation step (elements of array $A^{[2]}$).\n",
    "\n",
    "To minimize it, you can use gradient descent, updating the parameters with the following expressions:\n",
    "\n",
    "\\begin{align}\n",
    "W^{[1]} &= W^{[1]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]} },\\\\\n",
    "b^{[1]} &= b^{[1]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]} },\\\\\n",
    "W^{[2]} &= W^{[2]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} },\\\\\n",
    "b^{[2]} &= b^{[2]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} },\\\\\n",
    "\\tag{9}\n",
    "\\end{align}\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To perform training of the model you need to calculate now $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}}$, $\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}}$, $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]}}$, $\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]}}$. \n",
    "\n",
    "Let's start from the end of the neural network. You can rewrite here the corresponding expressions for $\\frac{\\partial \\mathcal{L} }{ \\partial W }$ and $\\frac{\\partial \\mathcal{L} }{ \\partial b }$ from the single perceptron neural network:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W } &= \n",
    "\\frac{1}{m}\\left(A-Y\\right)X^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b } &= \n",
    "\\frac{1}{m}\\left(A-Y\\right)\\mathbf{1},\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{1}$ is just a ($m \\times 1$) vector of ones. Your one perceptron is in the second layer now, so $W$ will be exchanged with $W^{[2]}$, $b$ with $b^{[2]}$, $A$ with $A^{[2]}$, $X$ with $A^{[1]}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\left(A^{[1]}\\right)^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\mathbf{1}.\\\\\n",
    "\\tag{10}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now find $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,1}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,1}^{[1]}} \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,2}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,2}^{[1]}} \\end{bmatrix}$. It was shown in the videos that $$\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,1}^{[1]}}=\\frac{1}{m}\\sum_{i=1}^{m} \\left( \n",
    "\\left(a^{[2](i)} - y^{(i)}\\right) \n",
    "w_1^{[2]} \n",
    "\\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right)\\tag{11}$$\n",
    "\n",
    "If you do this accurately for each of the elements $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}}$, you will get the following matrix:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,1}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,1}^{[1]}} \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,2}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,2}^{[1]}} \\end{bmatrix}$$\n",
    "$$= \\frac{1}{m}\\begin{bmatrix}\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)  \\\\\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)\\end{bmatrix}\\tag{12}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this, you can notice that all terms and indices somehow are very consistent, so it all can be unified into a matrix form. And that's true! $\\left(W^{[2]}\\right)^T = \\begin{bmatrix}w_1^{[2]} \\\\ w_2^{[2]}\\end{bmatrix}$ of size $\\left(n_h \\times n_y\\right) = \\left(2 \\times 1\\right)$ can be multiplied with the vector $A^{[2]} - Y$ of size $\\left(n_y \\times m\\right) = \\left(1 \\times m\\right)$, resulting in a matrix of size $\\left(n_h \\times m\\right) = \\left(2 \\times m\\right)$:\n",
    "\n",
    "$$\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)=\n",
    "\\begin{bmatrix}w_1^{[2]} \\\\ w_2^{[2]}\\end{bmatrix}\n",
    "\\begin{bmatrix}\\left(a^{[2](1)} - y^{(1)}\\right) &  \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right)\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_1^{[2]} & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_1^{[2]} \\\\\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_2^{[2]} & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_2^{[2]} \\end{bmatrix}$$.\n",
    "\n",
    "Now taking matrix $A^{[1]}$ of the same size $\\left(n_h \\times m\\right) = \\left(2 \\times m\\right)$,\n",
    "\n",
    "$$A^{[1]}\n",
    "=\\begin{bmatrix}\n",
    "a_1^{[1](1)} & \\cdots & a_1^{[1](m)} \\\\\n",
    "a_2^{[1](1)} & \\cdots & a_2^{[1](m)} \\end{bmatrix},$$\n",
    "\n",
    "you can calculate:\n",
    "\n",
    "$$A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\n",
    "=\\begin{bmatrix}\n",
    "a_1^{[1](1)}\\left(1 - a_1^{[1](1)}\\right) & \\cdots & a_1^{[1](m)}\\left(1 - a_1^{[1](m)}\\right) \\\\\n",
    "a_2^{[1](1)}\\left(1 - a_2^{[1](1)}\\right) & \\cdots & a_2^{[1](m)}\\left(1 - a_2^{[1](m)}\\right) \\end{bmatrix},$$\n",
    "\n",
    "where \"$\\cdot$\" denotes **element by element** multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the element by element multiplication,\n",
    "\n",
    "$$\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)=\\begin{bmatrix}\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_1^{[2]}\\left(a_1^{[1](1)}\\left(1 - a_1^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_1^{[2]}\\left(a_1^{[1](m)}\\left(1 - a_1^{[1](m)}\\right)\\right) \\\\\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_2^{[2]}\\left(a_2^{[1](1)}\\left(1 - a_2^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_2^{[2]} \\left(a_2^{[1](m)}\\left(1 - a_2^{[1](m)}\\right)\\right) \\end{bmatrix}.$$\n",
    "\n",
    "If you perform matrix multiplication with $X^T$ of size $\\left(m \\times n_x\\right) = \\left(m \\times 2\\right)$, you will get matrix of size $\\left(n_h \\times n_x\\right) = \\left(2 \\times 2\\right)$:\n",
    "\n",
    "$$\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T = \n",
    "\\begin{bmatrix}\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_1^{[2]}\\left(a_1^{[1](1)}\\left(1 - a_1^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_1^{[2]}\\left(a_1^{[1](m)}\\left(1 - a_1^{[1](m)}\\right)\\right) \\\\\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_2^{[2]}\\left(a_2^{[1](1)}\\left(1 - a_2^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_2^{[2]} \\left(a_2^{[1](m)}\\left(1 - a_2^{[1](m)}\\right)\\right) \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1^{(1)} & x_2^{(1)} \\\\\n",
    "\\cdots & \\cdots \\\\\n",
    "x_1^{(m)} & x_2^{(m)}\n",
    "\\end{bmatrix}$$\n",
    "$$=\\begin{bmatrix}\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1 - a_1^{[1](i)}\\right) \\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)  \\\\\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)\\end{bmatrix}$$\n",
    "\n",
    "This is exactly like in the expression $(12)$! So, $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}}$ can be written as a mixture of multiplications:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} = \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T\\tag{13},$$\n",
    "\n",
    "where \"$\\cdot$\" denotes element by element multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector $\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}}$ can be found very similarly, but the last terms in the chain rule will be equal to $1$, i.e. $\\frac{\\partial z_1^{[1](i)}}{ \\partial b_1^{[1]}} = 1$. Thus,\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}} = \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)\\mathbf{1},\\tag{14}$$\n",
    "\n",
    "where $\\mathbf{1}$ is a ($m \\times 1$) vector of ones.\n",
    "\n",
    "Expressions $(10)$, $(13)$ and $(14)$ can be used for the parameters update $(9)$ performing backward propagation:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\left(A^{[1]}\\right)^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\mathbf{1},\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)\\mathbf{1},\\\\\n",
    "\\tag{15}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{1}$ is a ($m \\times 1$) vector of ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to understand deeply and properly how neural networks perform and get trained, **you do need knowledge of linear algebra and calculus joined together**! But do not worry! All together it is not that scary if you do it step by step accurately with understanding of maths.\n",
    "\n",
    "Time to implement this all in the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Dataset\n",
    "\n",
    "First, let's get the dataset you will work on. The following code will create $m=2000$ data points $(x_1, x_2)$ and save them in the `NumPy` array `X` of a shape $(2 \\times m)$ (in the columns of the array). The labels ($0$: blue, $1$: red) will be saved in the `NumPy` array `Y` of a shape $(1 \\times m)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is: (2, 2000)\n",
      "The shape of Y is: (1, 2000)\n",
      "I have m = 2000 training examples!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 2000\n",
    "samples, labels = make_blobs(n_samples=m, \n",
    "                             centers=([2.5, 3], [6.7, 7.9], [2.1, 7.9], [7.4, 2.8]), \n",
    "                             cluster_std=1.1,\n",
    "                             random_state=0)\n",
    "labels[(labels == 0) | (labels == 1)] = 1\n",
    "labels[(labels == 2) | (labels == 3)] = 0\n",
    "X = np.transpose(samples)\n",
    "Y = labels.reshape((1, m))\n",
    "\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, cmap=colors.ListedColormap(['blue', 'red']));\n",
    "\n",
    "print ('The shape of X is: ' + str(X.shape))\n",
    "print ('The shape of Y is: ' + str(Y.shape))\n",
    "print ('I have m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - Define Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex01'></a>\n",
    "### Exercise 1\n",
    "\n",
    "Define sigmoid activation function $\\sigma\\left(z\\right) =\\frac{1}{1+e^{-z}} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ### START CODE HERE ### (~ 1 line of code)\n",
    "    res = 1/(1+np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(-2) = 0.11920292202211755\n",
      "sigmoid(0) = 0.5\n",
      "sigmoid(3.5) = 0.9706877692486436\n"
     ]
    }
   ],
   "source": [
    "print(\"sigmoid(-2) = \" + str(sigmoid(-2)))\n",
    "print(\"sigmoid(0) = \" + str(sigmoid(0)))\n",
    "print(\"sigmoid(3.5) = \" + str(sigmoid(3.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "Note: the values may vary in the last decimal places.\n",
    "\n",
    "```Python\n",
    "sigmoid(-2) = 0.11920292202211755\n",
    "sigmoid(0) = 0.5\n",
    "sigmoid(3.5) = 0.9706877692486436\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w3_unittest.test_sigmoid(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Implementation of the Neural Network Model with Two Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 - Defining the Neural Network Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex02'></a>\n",
    "### Exercise 2\n",
    "\n",
    "Define three variables:\n",
    "- `n_x`: the size of the input layer\n",
    "- `n_h`: the size of the hidden layer (set it equal to 2 for now)\n",
    "- `n_y`: the size of the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    Use shapes of X and Y to find n_x and n_y:\n",
    "    <li>the size of the input layer n_x equals to the size of the input vectors placed in the columns of the array X,</li>\n",
    "    <li>the outpus for each of the data point will be saved in the columns of the the array Y.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: layer_sizes\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (~ 3 lines of code)\n",
    "    # Size of input layer.\n",
    "    n_x = np.shape(X)[0]\n",
    "    # Size of hidden layer.\n",
    "    n_h = 2\n",
    "    # Size of output layer.\n",
    "    n_y = np.shape(Y)[0] \n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is: n_x = 2\n",
      "The size of the hidden layer is: n_h = 2\n",
      "The size of the output layer is: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "(n_x, n_h, n_y) = layer_sizes(X, Y)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```Python\n",
    "The size of the input layer is: n_x = 2\n",
    "The size of the hidden layer is: n_h = 2\n",
    "The size of the output layer is: n_y = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w3_unittest.test_layer_sizes(layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Initialize the Model's Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex03'></a>\n",
    "### Exercise 3\n",
    "\n",
    "Implement the function `initialize_parameters()`.\n",
    "\n",
    "**Instructions**:\n",
    "- Make sure your parameters' sizes are right. Refer to the neural network figure above if needed.\n",
    "- You will initialize the weights matrix with random values. \n",
    "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
    "- You will initialize the bias vector as zeros. \n",
    "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (~ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.00082741 -0.00627001]\n",
      " [-0.00043818 -0.00477218]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01313865  0.00884622]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "Note: the elements of the arrays W1 and W2 maybe be different due to random initialization. You can try to restart the kernel to get the same values.\n",
    "\n",
    "```Python\n",
    "W1 = [[ 0.01788628  0.0043651 ]\n",
    " [ 0.00096497 -0.01863493]]\n",
    "b1 = [[0.]\n",
    " [0.]]\n",
    "W2 = [[-0.00277388 -0.00354759]]\n",
    "b2 = [[0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Note: \n",
    "# Actual values are not checked here in the unit tests (due to random initialization).\n",
    "w3_unittest.test_initialize_parameters(initialize_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3 - The Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex04'></a>\n",
    "### Exercise 4\n",
    "\n",
    "Implement `forward_propagation()`.\n",
    "\n",
    "**Instructions**:\n",
    "- Look above at the mathematical representation $(7)$ of your classifier (section [2.2](#2.2)):\n",
    "\\begin{align}\n",
    "Z^{[1]} &= W^{[1]} X + b^{[1]},\\\\\n",
    "A^{[1]} &= \\sigma\\left(Z^{[1]}\\right),\\\\\n",
    "Z^{[2]} &= W^{[2]} A^{[1]} + b^{[2]},\\\\\n",
    "A^{[2]} &= \\sigma\\left(Z^{[2]}\\right).\\\\\n",
    "\\end{align}\n",
    "- The steps you have to implement are:\n",
    "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
    "    2. Implement Forward Propagation. Compute `Z1` multiplying matrices `W1`, `X` and adding vector `b1`. Then find `A1` using the `sigmoid` activation function. Perform similar computations for `Z2` and `A2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- the sigmoid output of the second activation\n",
    "    cache -- python dictionary containing Z1, A1, Z2, A2 \n",
    "    (that simplifies the calculations in the back propagation step)\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### (~ 4 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement forward propagation to calculate A2.\n",
    "    ### START CODE HERE ### (~ 4 lines of code)\n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A2.shape == (n_y, X.shape[1]))\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50003885 0.50011507 0.50009076 ... 0.50002754 0.5000427  0.50008169]]\n"
     ]
    }
   ],
   "source": [
    "A2, cache = forward_propagation(X, parameters)\n",
    "\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "Note: the elements of the array A2 maybe be different depending on the initial parameters. If you would like to get exactly the same output, try to restart the Kernel and rerun the notebook.\n",
    "\n",
    "```Python\n",
    "[[0.49920157 0.49922234 0.49921223 ... 0.49921215 0.49921043 0.49920665]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case \"default_check\". Wrong output of A1 for X = \n",
      "[[ 5.46584646  6.71120407  7.21301753 ...  1.77559174  3.52245562\n",
      "   7.86492998]\n",
      " [ 2.91868287 10.31812597  7.79616824 ...  2.43434264  3.64044705\n",
      "   6.77517917]]\n",
      "Test for i = 0, j = 0. \n",
      "\tExpected: \n",
      "0.5275979229090347\n",
      "\tGot: \n",
      "0.11005639586802393\n",
      "Test case \"default_check\". Wrong output of A1 for X = \n",
      "[[ 5.46584646  6.71120407  7.21301753 ...  1.77559174  3.52245562\n",
      "   7.86492998]\n",
      " [ 2.91868287 10.31812597  7.79616824 ...  2.43434264  3.64044705\n",
      "   6.77517917]]\n",
      "Test for i = 1, j = 1999. \n",
      "\tExpected: \n",
      "0.47036837134568177\n",
      "\tGot: \n",
      "-0.11811169057236745\n",
      "Test case \"default_check\". Wrong output of A1 for X = \n",
      "[[ 5.46584646  6.71120407  7.21301753 ...  1.77559174  3.52245562\n",
      "   7.86492998]\n",
      " [ 2.91868287 10.31812597  7.79616824 ...  2.43434264  3.64044705\n",
      "   6.77517917]]\n",
      "Test for i = 0, j = 100. \n",
      "\tExpected: \n",
      "0.521413303959268\n",
      "\tGot: \n",
      "0.08549640531158904\n",
      "Test case \"default_check\". Wrong output of Z2. Test for i = 0. \n",
      "\tExpected: \n",
      "-0.003193737045395555\n",
      "\tGot: \n",
      "-0.00013118306420127032\n",
      "Test case \"default_check\". Wrong output of Z2. Test for i = 400. \n",
      "\tExpected: \n",
      "-0.003221924688299396\n",
      "\tGot: \n",
      "-0.000243079266365854\n",
      "Test case \"default_check\". Wrong output of Z2. Test for i = 1999. \n",
      "\tExpected: \n",
      "-0.00317339213692169\n",
      "\tGot: \n",
      "-4.872717088260758e-05\n",
      "Test case \"default_check\". Wrong output of A2. Test for i = 0. \n",
      "\tExpected: \n",
      "0.4992015664173166\n",
      "\tGot: \n",
      "0.4999672042339967\n",
      "Test case \"default_check\". Wrong output of A2. Test for i = 400. \n",
      "\tExpected: \n",
      "0.49919451952471916\n",
      "\tGot: \n",
      "0.4999392301837078\n",
      "Test case \"default_check\". Wrong output of A2. Test for i = 1999. \n",
      "\tExpected: \n",
      "0.4992066526315478\n",
      "\tGot: \n",
      "0.4999878182072818\n",
      "Test case \"change_weights_check\". Wrong output of Z1 for X = \n",
      "[[ 5.46584646  6.71120407  7.21301753 ...  1.77559174  3.52245562\n",
      "   7.86492998]\n",
      " [ 2.91868287 10.31812597  7.79616824 ...  2.43434264  3.64044705\n",
      "   6.77517917]]\n",
      "Test for i = 1, j = 1999. \n",
      "\tExpected: \n",
      "-0.03577862954823146\n",
      "\tGot: \n",
      "-0.03577862954823145\n",
      "Test case \"change_weights_check\". Wrong output of A1 for X = \n",
      "[[ 5.46584646  6.71120407  7.21301753 ...  1.77559174  3.52245562\n",
      "   7.86492998]\n",
      " [ 2.91868287 10.31812597  7.79616824 ...  2.43434264  3.64044705\n",
      "   6.77517917]]\n",
      "Test for i = 0, j = 0. \n",
      "\tExpected: \n",
      "0.49429458095298745\n",
      "\tGot: \n",
      "-0.022818705027767668\n",
      "Test case \"change_weights_check\". Wrong output of A1 for X = \n",
      "[[ 5.46584646  6.71120407  7.21301753 ...  1.77559174  3.52245562\n",
      "   7.86492998]\n",
      " [ 2.91868287 10.31812597  7.79616824 ...  2.43434264  3.64044705\n",
      "   6.77517917]]\n",
      "Test for i = 1, j = 1999. \n",
      "\tExpected: \n",
      "0.4910562966698409\n",
      "\tGot: \n",
      "-0.03576337049703746\n",
      "Test case \"change_weights_check\". Wrong output of A1 for X = \n",
      "[[ 5.46584646  6.71120407  7.21301753 ...  1.77559174  3.52245562\n",
      "   7.86492998]\n",
      " [ 2.91868287 10.31812597  7.79616824 ...  2.43434264  3.64044705\n",
      "   6.77517917]]\n",
      "Test for i = 0, j = 100. \n",
      "\tExpected: \n",
      "0.4853224908106953\n",
      "\tGot: \n",
      "-0.05865948887234494\n",
      "Test case \"change_weights_check\". Wrong output of Z2. Test for i = 0. \n",
      "\tExpected: \n",
      "-0.0021073530226891173\n",
      "\tGot: \n",
      "0.00015541849010511672\n",
      "Test case \"change_weights_check\". Wrong output of Z2. Test for i = 400. \n",
      "\tExpected: \n",
      "-0.002110285690191978\n",
      "\tGot: \n",
      "0.00014369510943627084\n",
      "Test case \"change_weights_check\". Wrong output of Z2. Test for i = 1999. \n",
      "\tExpected: \n",
      "-0.0020644562143733863\n",
      "\tGot: \n",
      "0.00032675067858108537\n",
      "Test case \"change_weights_check\". Wrong output of A2. Test for i = 0. \n",
      "\tExpected: \n",
      "0.4994731619392989\n",
      "\tGot: \n",
      "0.500038854622448\n",
      "Test case \"change_weights_check\". Wrong output of A2. Test for i = 400. \n",
      "\tExpected: \n",
      "0.49947242877323833\n",
      "\tGot: \n",
      "0.5000359237772972\n",
      "Test case \"change_weights_check\". Wrong output of A2. Test for i = 1999. \n",
      "\tExpected: \n",
      "0.49948388612971223\n",
      "\tGot: \n",
      "0.5000816876689185\n",
      "Test case \"change_dataset_check\". Wrong output of A1 for X = \n",
      "[[0 1 0 0 1]\n",
      " [0 0 0 0 1]]\n",
      "Test for i = 0, j = 0. \n",
      "\tExpected: \n",
      "0.5\n",
      "\tGot: \n",
      "0.0\n",
      "Test case \"change_dataset_check\". Wrong output of A1 for X = \n",
      "[[0 1 0 0 1]\n",
      " [0 0 0 0 1]]\n",
      "Test for i = 1, j = 4. \n",
      "\tExpected: \n",
      "0.49869741294686865\n",
      "\tGot: \n",
      "-0.005210312850485785\n",
      "Test case \"change_dataset_check\". Wrong output of A1 for X = \n",
      "[[0 1 0 0 1]\n",
      " [0 0 0 0 1]]\n",
      "Test for i = 0, j = 4. \n",
      "\tExpected: \n",
      "0.49822565244831607\n",
      "\tGot: \n",
      "-0.007097300828745101\n",
      "Test case \"change_dataset_check\". Wrong output of Z2. Test for i = 0. \n",
      "\tExpected: \n",
      "-0.002146215\n",
      "\tGot: \n",
      "0.0\n",
      "Test case \"change_dataset_check\". Wrong output of Z2. Test for i = 1. \n",
      "\tExpected: \n",
      "-0.0021444662967103198\n",
      "\tGot: \n",
      "6.994811484181838e-06\n",
      "Test case \"change_dataset_check\". Wrong output of Z2. Test for i = 4. \n",
      "\tExpected: \n",
      "-0.00213442544018122\n",
      "\tGot: \n",
      "4.7157377789367467e-05\n",
      "Test case \"change_dataset_check\". Wrong output of A2. Test for i = 0. \n",
      "\tExpected: \n",
      "0.4994634464559578\n",
      "\tGot: \n",
      "0.5\n",
      "Test case \"change_dataset_check\". Wrong output of A2. Test for i = 1. \n",
      "\tExpected: \n",
      "0.4994638836312772\n",
      "\tGot: \n",
      "0.5000017487028711\n",
      "Test case \"change_dataset_check\". Wrong output of A2. Test for i = 4. \n",
      "\tExpected: \n",
      "0.49946639384253705\n",
      "\tGot: \n",
      "0.5000117893444451\n",
      "\u001b[92m 20  Tests passed\n",
      "\u001b[91m 28  Tests failed\n"
     ]
    }
   ],
   "source": [
    "# Note: \n",
    "# Actual values are not checked here in the unit tests (due to random initialization).\n",
    "w3_unittest.test_forward_propagation(forward_propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, that your weights were just initialized with some random values, so the model has not been trained yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex05'></a>\n",
    "### Exercise 5\n",
    "\n",
    "Define a cost function $(8)$ which will be used to train the model:\n",
    "\n",
    "$$\\mathcal{L}\\left(W, b\\right)  = \\frac{1}{m}\\sum_{i=1}^{m}  \\large\\left(\\small - y^{(i)}\\log\\left(a^{(i)}\\right) - (1-y^{(i)})\\log\\left(1- a^{(i)}\\right)  \\large  \\right) \\small.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost function as a log loss\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The output of the neural network of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- log loss\n",
    "    \n",
    "    \"\"\"\n",
    "    # Number of examples.\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    ### START CODE HERE ### (~ 2 lines of code)\n",
    "    logloss = np.multiply(np.log(A2),Y)+np.multiply(np.log(1-A2),1-Y)\n",
    "    cost = -np.sum(logloss)/m\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.6931464484289974\n"
     ]
    }
   ],
   "source": [
    "print(\"cost = \" + str(compute_cost(A2, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "Note: the elements of the arrays W1 and W2 maybe be different!\n",
    "\n",
    "```Python\n",
    "cost = 0.6931477703826823\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Note: \n",
    "# Actual values are not checked here in the unit tests (due to random initialization).\n",
    "w3_unittest.test_compute_cost(compute_cost, A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate partial derivatives as shown in $(15)$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\left(A^{[1]}\\right)^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\mathbf{1},\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)\\mathbf{1}.\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[-2.54641499e-04 -5.18803911e-05]\n",
      " [ 1.29294783e-04  2.03904836e-05]]\n",
      "db1 = [[-2.06620273e-06]\n",
      " [ 1.00152389e-06]]\n",
      "dW2 = [[1.20342313e-04 9.70144312e-05]]\n",
      "db2 = [[6.21707711e-05]]\n"
     ]
    }
   ],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation, calculating gradients\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- python dictionary containing Z1, A1, Z2, A2\n",
    "    X -- input data of shape (n_x, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (n_y, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculate partial derivatives denoted as dW1, db1, dW2, db2 for simplicity. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * A1 * (1 - A1)\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "grads = backward_propagation(parameters, cache, X, Y)\n",
    "\n",
    "print(\"dW1 = \" + str(grads[\"dW1\"]))\n",
    "print(\"db1 = \" + str(grads[\"db1\"]))\n",
    "print(\"dW2 = \" + str(grads[\"dW2\"]))\n",
    "print(\"db2 = \" + str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex06'></a>\n",
    "### Exercise 6\n",
    "\n",
    "Implement `update_parameters()`.\n",
    "\n",
    "**Instructions**:\n",
    "- Update parameters as shown in $(9)$ (section [2.3](#2.3)):\n",
    "\\begin{align}\n",
    "W^{[1]} &= W^{[1]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]} },\\\\\n",
    "b^{[1]} &= b^{[1]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]} },\\\\\n",
    "W^{[2]} &= W^{[2]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} },\\\\\n",
    "b^{[2]} &= b^{[2]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} }.\\\\\n",
    "\\end{align}\n",
    "- The steps you have to implement are:\n",
    "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
    "    2. Retrieve each derivative from the dictionary \"grads\" (which is the output of `backward_propagation()`) by using `grads[\"..\"]`.\n",
    "    3. Update parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing parameters \n",
    "    grads -- python dictionary containing gradients\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### (~ 4 lines of code)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\".\n",
    "    ### START CODE HERE ### (~ 4 lines of code)\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter.\n",
    "    ### START CODE HERE ### (~ 4 lines of code)\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 updated = [[-0.00052185 -0.00620775]\n",
      " [-0.00059334 -0.00479665]]\n",
      "b1 updated = [[ 2.47944327e-06]\n",
      " [-1.20182867e-06]]\n",
      "W2 updated = [[-0.01328306  0.00872981]]\n",
      "b2 updated = [[-7.46049254e-05]]\n"
     ]
    }
   ],
   "source": [
    "parameters_updated = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 updated = \" + str(parameters_updated[\"W1\"]))\n",
    "print(\"b1 updated = \" + str(parameters_updated[\"b1\"]))\n",
    "print(\"W2 updated = \" + str(parameters_updated[\"W2\"]))\n",
    "print(\"b2 updated = \" + str(parameters_updated[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "Note: the actual values can be different!\n",
    "\n",
    "```Python\n",
    "W1 updated = [[ 0.01790427  0.00434496]\n",
    " [ 0.00099046 -0.01866419]]\n",
    "b1 updated = [[-6.13449205e-07]\n",
    " [-8.47483463e-07]]\n",
    "W2 updated = [[-0.00238219 -0.00323487]]\n",
    "b2 updated = [[0.00094478]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w3_unittest.test_update_parameters(update_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 - Integrate parts 3.1, 3.2 and 3.3 in nn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex07'></a>\n",
    "### Exercise 7\n",
    "\n",
    "Build your neural network model in `nn_model()`.\n",
    "\n",
    "**Instructions**: The neural network model has to use the previous functions in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations=10, learning_rate=1.2, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (n_x, number of examples)\n",
    "    Y -- labels of shape (n_y, number of examples)\n",
    "    num_iterations -- number of iterations in the loop\n",
    "    learning_rate -- learning rate parameter for gradient descent\n",
    "    print_cost -- if True, print the cost every iteration\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters.\n",
    "    ### START CODE HERE ### (~ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop.\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### (~ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache =  forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads, learning_rate\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every iteration.\n",
    "        if print_cost:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693158\n",
      "Cost after iteration 1: 0.693156\n",
      "Cost after iteration 2: 0.693155\n",
      "Cost after iteration 3: 0.693153\n",
      "Cost after iteration 4: 0.693152\n",
      "Cost after iteration 5: 0.693151\n",
      "Cost after iteration 6: 0.693150\n",
      "Cost after iteration 7: 0.693148\n",
      "Cost after iteration 8: 0.693147\n",
      "Cost after iteration 9: 0.693146\n",
      "Cost after iteration 10: 0.693144\n",
      "Cost after iteration 11: 0.693142\n",
      "Cost after iteration 12: 0.693140\n",
      "Cost after iteration 13: 0.693136\n",
      "Cost after iteration 14: 0.693132\n",
      "Cost after iteration 15: 0.693127\n",
      "Cost after iteration 16: 0.693119\n",
      "Cost after iteration 17: 0.693107\n",
      "Cost after iteration 18: 0.693089\n",
      "Cost after iteration 19: 0.693062\n",
      "Cost after iteration 20: 0.693019\n",
      "Cost after iteration 21: 0.692950\n",
      "Cost after iteration 22: 0.692839\n",
      "Cost after iteration 23: 0.692666\n",
      "Cost after iteration 24: 0.692416\n",
      "Cost after iteration 25: 0.692091\n",
      "Cost after iteration 26: 0.691721\n",
      "Cost after iteration 27: 0.691352\n",
      "Cost after iteration 28: 0.691020\n",
      "Cost after iteration 29: 0.690742\n",
      "Cost after iteration 30: 0.690522\n",
      "Cost after iteration 31: 0.690354\n",
      "Cost after iteration 32: 0.690231\n",
      "Cost after iteration 33: 0.690145\n",
      "Cost after iteration 34: 0.690088\n",
      "Cost after iteration 35: 0.690055\n",
      "Cost after iteration 36: 0.690041\n",
      "Cost after iteration 37: 0.690042\n",
      "Cost after iteration 38: 0.690056\n",
      "Cost after iteration 39: 0.690080\n",
      "Cost after iteration 40: 0.690110\n",
      "Cost after iteration 41: 0.690142\n",
      "Cost after iteration 42: 0.690170\n",
      "Cost after iteration 43: 0.690182\n",
      "Cost after iteration 44: 0.690159\n",
      "Cost after iteration 45: 0.690057\n",
      "Cost after iteration 46: 0.689778\n",
      "Cost after iteration 47: 0.689088\n",
      "Cost after iteration 48: 0.687438\n",
      "Cost after iteration 49: 0.683745\n",
      "Cost after iteration 50: 0.676689\n",
      "Cost after iteration 51: 0.665911\n",
      "Cost after iteration 52: 0.654262\n",
      "Cost after iteration 53: 0.644645\n",
      "Cost after iteration 54: 0.655046\n",
      "Cost after iteration 55: 0.631337\n",
      "Cost after iteration 56: 0.681787\n",
      "Cost after iteration 57: 0.627372\n",
      "Cost after iteration 58: 0.674660\n",
      "Cost after iteration 59: 0.641193\n",
      "Cost after iteration 60: 0.660989\n",
      "Cost after iteration 61: 0.652679\n",
      "Cost after iteration 62: 0.657260\n",
      "Cost after iteration 63: 0.654071\n",
      "Cost after iteration 64: 0.653845\n",
      "Cost after iteration 65: 0.652245\n",
      "Cost after iteration 66: 0.650654\n",
      "Cost after iteration 67: 0.648864\n",
      "Cost after iteration 68: 0.646946\n",
      "Cost after iteration 69: 0.644940\n",
      "Cost after iteration 70: 0.642886\n",
      "Cost after iteration 71: 0.640821\n",
      "Cost after iteration 72: 0.638786\n",
      "Cost after iteration 73: 0.636827\n",
      "Cost after iteration 74: 0.635001\n",
      "Cost after iteration 75: 0.633386\n",
      "Cost after iteration 76: 0.632091\n",
      "Cost after iteration 77: 0.631272\n",
      "Cost after iteration 78: 0.631123\n",
      "Cost after iteration 79: 0.631831\n",
      "Cost after iteration 80: 0.633409\n",
      "Cost after iteration 81: 0.635451\n",
      "Cost after iteration 82: 0.637382\n",
      "Cost after iteration 83: 0.639560\n",
      "Cost after iteration 84: 0.642012\n",
      "Cost after iteration 85: 0.644540\n",
      "Cost after iteration 86: 0.646658\n",
      "Cost after iteration 87: 0.648363\n",
      "Cost after iteration 88: 0.649929\n",
      "Cost after iteration 89: 0.651466\n",
      "Cost after iteration 90: 0.652950\n",
      "Cost after iteration 91: 0.654429\n",
      "Cost after iteration 92: 0.655770\n",
      "Cost after iteration 93: 0.656956\n",
      "Cost after iteration 94: 0.658027\n",
      "Cost after iteration 95: 0.658977\n",
      "Cost after iteration 96: 0.659792\n",
      "Cost after iteration 97: 0.660536\n",
      "Cost after iteration 98: 0.661395\n",
      "Cost after iteration 99: 0.662411\n",
      "Cost after iteration 100: 0.663340\n",
      "Cost after iteration 101: 0.664124\n",
      "Cost after iteration 102: 0.664815\n",
      "Cost after iteration 103: 0.665434\n",
      "Cost after iteration 104: 0.665988\n",
      "Cost after iteration 105: 0.666507\n",
      "Cost after iteration 106: 0.666989\n",
      "Cost after iteration 107: 0.667287\n",
      "Cost after iteration 108: 0.667238\n",
      "Cost after iteration 109: 0.666733\n",
      "Cost after iteration 110: 0.665536\n",
      "Cost after iteration 111: 0.662932\n",
      "Cost after iteration 112: 0.657860\n",
      "Cost after iteration 113: 0.649803\n",
      "Cost after iteration 114: 0.638383\n",
      "Cost after iteration 115: 0.626907\n",
      "Cost after iteration 116: 0.623302\n",
      "Cost after iteration 117: 0.632943\n",
      "Cost after iteration 118: 0.644581\n",
      "Cost after iteration 119: 0.650215\n",
      "Cost after iteration 120: 0.655774\n",
      "Cost after iteration 121: 0.660032\n",
      "Cost after iteration 122: 0.663324\n",
      "Cost after iteration 123: 0.665851\n",
      "Cost after iteration 124: 0.667686\n",
      "Cost after iteration 125: 0.668887\n",
      "Cost after iteration 126: 0.669574\n",
      "Cost after iteration 127: 0.669915\n",
      "Cost after iteration 128: 0.670043\n",
      "Cost after iteration 129: 0.670050\n",
      "Cost after iteration 130: 0.669994\n",
      "Cost after iteration 131: 0.669913\n",
      "Cost after iteration 132: 0.669830\n",
      "Cost after iteration 133: 0.669758\n",
      "Cost after iteration 134: 0.669703\n",
      "Cost after iteration 135: 0.669669\n",
      "Cost after iteration 136: 0.669654\n",
      "Cost after iteration 137: 0.669657\n",
      "Cost after iteration 138: 0.669677\n",
      "Cost after iteration 139: 0.669712\n",
      "Cost after iteration 140: 0.669760\n",
      "Cost after iteration 141: 0.669820\n",
      "Cost after iteration 142: 0.669892\n",
      "Cost after iteration 143: 0.669976\n",
      "Cost after iteration 144: 0.670071\n",
      "Cost after iteration 145: 0.670178\n",
      "Cost after iteration 146: 0.670296\n",
      "Cost after iteration 147: 0.670425\n",
      "Cost after iteration 148: 0.670564\n",
      "Cost after iteration 149: 0.670713\n",
      "Cost after iteration 150: 0.670871\n",
      "Cost after iteration 151: 0.671039\n",
      "Cost after iteration 152: 0.671219\n",
      "Cost after iteration 153: 0.671409\n",
      "Cost after iteration 154: 0.671607\n",
      "Cost after iteration 155: 0.671811\n",
      "Cost after iteration 156: 0.672019\n",
      "Cost after iteration 157: 0.672227\n",
      "Cost after iteration 158: 0.672436\n",
      "Cost after iteration 159: 0.672643\n",
      "Cost after iteration 160: 0.672848\n",
      "Cost after iteration 161: 0.673050\n",
      "Cost after iteration 162: 0.673247\n",
      "Cost after iteration 163: 0.673435\n",
      "Cost after iteration 164: 0.673615\n",
      "Cost after iteration 165: 0.673787\n",
      "Cost after iteration 166: 0.673956\n",
      "Cost after iteration 167: 0.674123\n",
      "Cost after iteration 168: 0.674291\n",
      "Cost after iteration 169: 0.674459\n",
      "Cost after iteration 170: 0.674624\n",
      "Cost after iteration 171: 0.674784\n",
      "Cost after iteration 172: 0.674938\n",
      "Cost after iteration 173: 0.675084\n",
      "Cost after iteration 174: 0.675224\n",
      "Cost after iteration 175: 0.675361\n",
      "Cost after iteration 176: 0.675499\n",
      "Cost after iteration 177: 0.675641\n",
      "Cost after iteration 178: 0.675788\n",
      "Cost after iteration 179: 0.675939\n",
      "Cost after iteration 180: 0.676094\n",
      "Cost after iteration 181: 0.676251\n",
      "Cost after iteration 182: 0.676410\n",
      "Cost after iteration 183: 0.676568\n",
      "Cost after iteration 184: 0.676725\n",
      "Cost after iteration 185: 0.676880\n",
      "Cost after iteration 186: 0.677032\n",
      "Cost after iteration 187: 0.677181\n",
      "Cost after iteration 188: 0.677328\n",
      "Cost after iteration 189: 0.677472\n",
      "Cost after iteration 190: 0.677613\n",
      "Cost after iteration 191: 0.677751\n",
      "Cost after iteration 192: 0.677884\n",
      "Cost after iteration 193: 0.678013\n",
      "Cost after iteration 194: 0.678138\n",
      "Cost after iteration 195: 0.678259\n",
      "Cost after iteration 196: 0.678377\n",
      "Cost after iteration 197: 0.678492\n",
      "Cost after iteration 198: 0.678602\n",
      "Cost after iteration 199: 0.678708\n",
      "Cost after iteration 200: 0.678811\n",
      "Cost after iteration 201: 0.678909\n",
      "Cost after iteration 202: 0.679004\n",
      "Cost after iteration 203: 0.679095\n",
      "Cost after iteration 204: 0.679182\n",
      "Cost after iteration 205: 0.679266\n",
      "Cost after iteration 206: 0.679346\n",
      "Cost after iteration 207: 0.679423\n",
      "Cost after iteration 208: 0.679496\n",
      "Cost after iteration 209: 0.679564\n",
      "Cost after iteration 210: 0.679628\n",
      "Cost after iteration 211: 0.679688\n",
      "Cost after iteration 212: 0.679743\n",
      "Cost after iteration 213: 0.679793\n",
      "Cost after iteration 214: 0.679840\n",
      "Cost after iteration 215: 0.679885\n",
      "Cost after iteration 216: 0.679927\n",
      "Cost after iteration 217: 0.679969\n",
      "Cost after iteration 218: 0.680011\n",
      "Cost after iteration 219: 0.680053\n",
      "Cost after iteration 220: 0.680096\n",
      "Cost after iteration 221: 0.680140\n",
      "Cost after iteration 222: 0.680186\n",
      "Cost after iteration 223: 0.680233\n",
      "Cost after iteration 224: 0.680281\n",
      "Cost after iteration 225: 0.680329\n",
      "Cost after iteration 226: 0.680379\n",
      "Cost after iteration 227: 0.680428\n",
      "Cost after iteration 228: 0.680478\n",
      "Cost after iteration 229: 0.680527\n",
      "Cost after iteration 230: 0.680576\n",
      "Cost after iteration 231: 0.680624\n",
      "Cost after iteration 232: 0.680670\n",
      "Cost after iteration 233: 0.680716\n",
      "Cost after iteration 234: 0.680760\n",
      "Cost after iteration 235: 0.680802\n",
      "Cost after iteration 236: 0.680843\n",
      "Cost after iteration 237: 0.680882\n",
      "Cost after iteration 238: 0.680920\n",
      "Cost after iteration 239: 0.680956\n",
      "Cost after iteration 240: 0.680991\n",
      "Cost after iteration 241: 0.681025\n",
      "Cost after iteration 242: 0.681058\n",
      "Cost after iteration 243: 0.681090\n",
      "Cost after iteration 244: 0.681121\n",
      "Cost after iteration 245: 0.681151\n",
      "Cost after iteration 246: 0.681181\n",
      "Cost after iteration 247: 0.681210\n",
      "Cost after iteration 248: 0.681238\n",
      "Cost after iteration 249: 0.681266\n",
      "Cost after iteration 250: 0.681294\n",
      "Cost after iteration 251: 0.681322\n",
      "Cost after iteration 252: 0.681349\n",
      "Cost after iteration 253: 0.681376\n",
      "Cost after iteration 254: 0.681403\n",
      "Cost after iteration 255: 0.681430\n",
      "Cost after iteration 256: 0.681457\n",
      "Cost after iteration 257: 0.681484\n",
      "Cost after iteration 258: 0.681511\n",
      "Cost after iteration 259: 0.681538\n",
      "Cost after iteration 260: 0.681565\n",
      "Cost after iteration 261: 0.681593\n",
      "Cost after iteration 262: 0.681622\n",
      "Cost after iteration 263: 0.681651\n",
      "Cost after iteration 264: 0.681682\n",
      "Cost after iteration 265: 0.681713\n",
      "Cost after iteration 266: 0.681745\n",
      "Cost after iteration 267: 0.681778\n",
      "Cost after iteration 268: 0.681811\n",
      "Cost after iteration 269: 0.681844\n",
      "Cost after iteration 270: 0.681878\n",
      "Cost after iteration 271: 0.681913\n",
      "Cost after iteration 272: 0.681947\n",
      "Cost after iteration 273: 0.681981\n",
      "Cost after iteration 274: 0.682015\n",
      "Cost after iteration 275: 0.682050\n",
      "Cost after iteration 276: 0.682084\n",
      "Cost after iteration 277: 0.682117\n",
      "Cost after iteration 278: 0.682151\n",
      "Cost after iteration 279: 0.682184\n",
      "Cost after iteration 280: 0.682217\n",
      "Cost after iteration 281: 0.682250\n",
      "Cost after iteration 282: 0.682283\n",
      "Cost after iteration 283: 0.682315\n",
      "Cost after iteration 284: 0.682347\n",
      "Cost after iteration 285: 0.682379\n",
      "Cost after iteration 286: 0.682410\n",
      "Cost after iteration 287: 0.682441\n",
      "Cost after iteration 288: 0.682472\n",
      "Cost after iteration 289: 0.682503\n",
      "Cost after iteration 290: 0.682534\n",
      "Cost after iteration 291: 0.682564\n",
      "Cost after iteration 292: 0.682594\n",
      "Cost after iteration 293: 0.682623\n",
      "Cost after iteration 294: 0.682652\n",
      "Cost after iteration 295: 0.682681\n",
      "Cost after iteration 296: 0.682710\n",
      "Cost after iteration 297: 0.682739\n",
      "Cost after iteration 298: 0.682767\n",
      "Cost after iteration 299: 0.682795\n",
      "Cost after iteration 300: 0.682822\n",
      "Cost after iteration 301: 0.682849\n",
      "Cost after iteration 302: 0.682876\n",
      "Cost after iteration 303: 0.682903\n",
      "Cost after iteration 304: 0.682929\n",
      "Cost after iteration 305: 0.682955\n",
      "Cost after iteration 306: 0.682981\n",
      "Cost after iteration 307: 0.683006\n",
      "Cost after iteration 308: 0.683031\n",
      "Cost after iteration 309: 0.683056\n",
      "Cost after iteration 310: 0.683081\n",
      "Cost after iteration 311: 0.683105\n",
      "Cost after iteration 312: 0.683129\n",
      "Cost after iteration 313: 0.683152\n",
      "Cost after iteration 314: 0.683176\n",
      "Cost after iteration 315: 0.683199\n",
      "Cost after iteration 316: 0.683222\n",
      "Cost after iteration 317: 0.683245\n",
      "Cost after iteration 318: 0.683267\n",
      "Cost after iteration 319: 0.683290\n",
      "Cost after iteration 320: 0.683312\n",
      "Cost after iteration 321: 0.683333\n",
      "Cost after iteration 322: 0.683355\n",
      "Cost after iteration 323: 0.683376\n",
      "Cost after iteration 324: 0.683398\n",
      "Cost after iteration 325: 0.683419\n",
      "Cost after iteration 326: 0.683440\n",
      "Cost after iteration 327: 0.683460\n",
      "Cost after iteration 328: 0.683481\n",
      "Cost after iteration 329: 0.683501\n",
      "Cost after iteration 330: 0.683522\n",
      "Cost after iteration 331: 0.683542\n",
      "Cost after iteration 332: 0.683562\n",
      "Cost after iteration 333: 0.683581\n",
      "Cost after iteration 334: 0.683601\n",
      "Cost after iteration 335: 0.683620\n",
      "Cost after iteration 336: 0.683640\n",
      "Cost after iteration 337: 0.683659\n",
      "Cost after iteration 338: 0.683678\n",
      "Cost after iteration 339: 0.683697\n",
      "Cost after iteration 340: 0.683716\n",
      "Cost after iteration 341: 0.683734\n",
      "Cost after iteration 342: 0.683753\n",
      "Cost after iteration 343: 0.683771\n",
      "Cost after iteration 344: 0.683789\n",
      "Cost after iteration 345: 0.683807\n",
      "Cost after iteration 346: 0.683825\n",
      "Cost after iteration 347: 0.683843\n",
      "Cost after iteration 348: 0.683861\n",
      "Cost after iteration 349: 0.683878\n",
      "Cost after iteration 350: 0.683896\n",
      "Cost after iteration 351: 0.683913\n",
      "Cost after iteration 352: 0.683930\n",
      "Cost after iteration 353: 0.683948\n",
      "Cost after iteration 354: 0.683965\n",
      "Cost after iteration 355: 0.683982\n",
      "Cost after iteration 356: 0.683998\n",
      "Cost after iteration 357: 0.684015\n",
      "Cost after iteration 358: 0.684032\n",
      "Cost after iteration 359: 0.684048\n",
      "Cost after iteration 360: 0.684065\n",
      "Cost after iteration 361: 0.684081\n",
      "Cost after iteration 362: 0.684097\n",
      "Cost after iteration 363: 0.684114\n",
      "Cost after iteration 364: 0.684130\n",
      "Cost after iteration 365: 0.684146\n",
      "Cost after iteration 366: 0.684162\n",
      "Cost after iteration 367: 0.684177\n",
      "Cost after iteration 368: 0.684193\n",
      "Cost after iteration 369: 0.684209\n",
      "Cost after iteration 370: 0.684224\n",
      "Cost after iteration 371: 0.684240\n",
      "Cost after iteration 372: 0.684255\n",
      "Cost after iteration 373: 0.684270\n",
      "Cost after iteration 374: 0.684286\n",
      "Cost after iteration 375: 0.684301\n",
      "Cost after iteration 376: 0.684316\n",
      "Cost after iteration 377: 0.684331\n",
      "Cost after iteration 378: 0.684346\n",
      "Cost after iteration 379: 0.684360\n",
      "Cost after iteration 380: 0.684375\n",
      "Cost after iteration 381: 0.684390\n",
      "Cost after iteration 382: 0.684404\n",
      "Cost after iteration 383: 0.684419\n",
      "Cost after iteration 384: 0.684433\n",
      "Cost after iteration 385: 0.684448\n",
      "Cost after iteration 386: 0.684462\n",
      "Cost after iteration 387: 0.684476\n",
      "Cost after iteration 388: 0.684490\n",
      "Cost after iteration 389: 0.684504\n",
      "Cost after iteration 390: 0.684518\n",
      "Cost after iteration 391: 0.684532\n",
      "Cost after iteration 392: 0.684546\n",
      "Cost after iteration 393: 0.684560\n",
      "Cost after iteration 394: 0.684574\n",
      "Cost after iteration 395: 0.684588\n",
      "Cost after iteration 396: 0.684601\n",
      "Cost after iteration 397: 0.684615\n",
      "Cost after iteration 398: 0.684628\n",
      "Cost after iteration 399: 0.684641\n",
      "Cost after iteration 400: 0.684655\n",
      "Cost after iteration 401: 0.684668\n",
      "Cost after iteration 402: 0.684681\n",
      "Cost after iteration 403: 0.684694\n",
      "Cost after iteration 404: 0.684707\n",
      "Cost after iteration 405: 0.684720\n",
      "Cost after iteration 406: 0.684733\n",
      "Cost after iteration 407: 0.684746\n",
      "Cost after iteration 408: 0.684759\n",
      "Cost after iteration 409: 0.684771\n",
      "Cost after iteration 410: 0.684784\n",
      "Cost after iteration 411: 0.684796\n",
      "Cost after iteration 412: 0.684809\n",
      "Cost after iteration 413: 0.684821\n",
      "Cost after iteration 414: 0.684834\n",
      "Cost after iteration 415: 0.684846\n",
      "Cost after iteration 416: 0.684858\n",
      "Cost after iteration 417: 0.684871\n",
      "Cost after iteration 418: 0.684883\n",
      "Cost after iteration 419: 0.684895\n",
      "Cost after iteration 420: 0.684907\n",
      "Cost after iteration 421: 0.684919\n",
      "Cost after iteration 422: 0.684931\n",
      "Cost after iteration 423: 0.684943\n",
      "Cost after iteration 424: 0.684954\n",
      "Cost after iteration 425: 0.684966\n",
      "Cost after iteration 426: 0.684978\n",
      "Cost after iteration 427: 0.684990\n",
      "Cost after iteration 428: 0.685001\n",
      "Cost after iteration 429: 0.685013\n",
      "Cost after iteration 430: 0.685024\n",
      "Cost after iteration 431: 0.685036\n",
      "Cost after iteration 432: 0.685047\n",
      "Cost after iteration 433: 0.685058\n",
      "Cost after iteration 434: 0.685070\n",
      "Cost after iteration 435: 0.685081\n",
      "Cost after iteration 436: 0.685092\n",
      "Cost after iteration 437: 0.685103\n",
      "Cost after iteration 438: 0.685114\n",
      "Cost after iteration 439: 0.685125\n",
      "Cost after iteration 440: 0.685136\n",
      "Cost after iteration 441: 0.685147\n",
      "Cost after iteration 442: 0.685158\n",
      "Cost after iteration 443: 0.685169\n",
      "Cost after iteration 444: 0.685179\n",
      "Cost after iteration 445: 0.685190\n",
      "Cost after iteration 446: 0.685200\n",
      "Cost after iteration 447: 0.685211\n",
      "Cost after iteration 448: 0.685221\n",
      "Cost after iteration 449: 0.685231\n",
      "Cost after iteration 450: 0.685242\n",
      "Cost after iteration 451: 0.685252\n",
      "Cost after iteration 452: 0.685262\n",
      "Cost after iteration 453: 0.685272\n",
      "Cost after iteration 454: 0.685282\n",
      "Cost after iteration 455: 0.685292\n",
      "Cost after iteration 456: 0.685301\n",
      "Cost after iteration 457: 0.685311\n",
      "Cost after iteration 458: 0.685321\n",
      "Cost after iteration 459: 0.685330\n",
      "Cost after iteration 460: 0.685340\n",
      "Cost after iteration 461: 0.685349\n",
      "Cost after iteration 462: 0.685359\n",
      "Cost after iteration 463: 0.685368\n",
      "Cost after iteration 464: 0.685377\n",
      "Cost after iteration 465: 0.685386\n",
      "Cost after iteration 466: 0.685395\n",
      "Cost after iteration 467: 0.685404\n",
      "Cost after iteration 468: 0.685413\n",
      "Cost after iteration 469: 0.685422\n",
      "Cost after iteration 470: 0.685431\n",
      "Cost after iteration 471: 0.685439\n",
      "Cost after iteration 472: 0.685448\n",
      "Cost after iteration 473: 0.685457\n",
      "Cost after iteration 474: 0.685465\n",
      "Cost after iteration 475: 0.685473\n",
      "Cost after iteration 476: 0.685482\n",
      "Cost after iteration 477: 0.685490\n",
      "Cost after iteration 478: 0.685498\n",
      "Cost after iteration 479: 0.685506\n",
      "Cost after iteration 480: 0.685514\n",
      "Cost after iteration 481: 0.685522\n",
      "Cost after iteration 482: 0.685530\n",
      "Cost after iteration 483: 0.685538\n",
      "Cost after iteration 484: 0.685546\n",
      "Cost after iteration 485: 0.685554\n",
      "Cost after iteration 486: 0.685562\n",
      "Cost after iteration 487: 0.685569\n",
      "Cost after iteration 488: 0.685577\n",
      "Cost after iteration 489: 0.685584\n",
      "Cost after iteration 490: 0.685592\n",
      "Cost after iteration 491: 0.685599\n",
      "Cost after iteration 492: 0.685606\n",
      "Cost after iteration 493: 0.685614\n",
      "Cost after iteration 494: 0.685621\n",
      "Cost after iteration 495: 0.685628\n",
      "Cost after iteration 496: 0.685635\n",
      "Cost after iteration 497: 0.685642\n",
      "Cost after iteration 498: 0.685649\n",
      "Cost after iteration 499: 0.685656\n",
      "Cost after iteration 500: 0.685663\n",
      "Cost after iteration 501: 0.685670\n",
      "Cost after iteration 502: 0.685677\n",
      "Cost after iteration 503: 0.685684\n",
      "Cost after iteration 504: 0.685691\n",
      "Cost after iteration 505: 0.685697\n",
      "Cost after iteration 506: 0.685704\n",
      "Cost after iteration 507: 0.685711\n",
      "Cost after iteration 508: 0.685717\n",
      "Cost after iteration 509: 0.685724\n",
      "Cost after iteration 510: 0.685730\n",
      "Cost after iteration 511: 0.685737\n",
      "Cost after iteration 512: 0.685743\n",
      "Cost after iteration 513: 0.685750\n",
      "Cost after iteration 514: 0.685756\n",
      "Cost after iteration 515: 0.685762\n",
      "Cost after iteration 516: 0.685768\n",
      "Cost after iteration 517: 0.685775\n",
      "Cost after iteration 518: 0.685781\n",
      "Cost after iteration 519: 0.685787\n",
      "Cost after iteration 520: 0.685793\n",
      "Cost after iteration 521: 0.685799\n",
      "Cost after iteration 522: 0.685805\n",
      "Cost after iteration 523: 0.685811\n",
      "Cost after iteration 524: 0.685817\n",
      "Cost after iteration 525: 0.685823\n",
      "Cost after iteration 526: 0.685829\n",
      "Cost after iteration 527: 0.685835\n",
      "Cost after iteration 528: 0.685841\n",
      "Cost after iteration 529: 0.685847\n",
      "Cost after iteration 530: 0.685852\n",
      "Cost after iteration 531: 0.685858\n",
      "Cost after iteration 532: 0.685864\n",
      "Cost after iteration 533: 0.685870\n",
      "Cost after iteration 534: 0.685875\n",
      "Cost after iteration 535: 0.685881\n",
      "Cost after iteration 536: 0.685886\n",
      "Cost after iteration 537: 0.685892\n",
      "Cost after iteration 538: 0.685898\n",
      "Cost after iteration 539: 0.685903\n",
      "Cost after iteration 540: 0.685909\n",
      "Cost after iteration 541: 0.685914\n",
      "Cost after iteration 542: 0.685919\n",
      "Cost after iteration 543: 0.685925\n",
      "Cost after iteration 544: 0.685930\n",
      "Cost after iteration 545: 0.685936\n",
      "Cost after iteration 546: 0.685941\n",
      "Cost after iteration 547: 0.685946\n",
      "Cost after iteration 548: 0.685952\n",
      "Cost after iteration 549: 0.685957\n",
      "Cost after iteration 550: 0.685962\n",
      "Cost after iteration 551: 0.685967\n",
      "Cost after iteration 552: 0.685973\n",
      "Cost after iteration 553: 0.685978\n",
      "Cost after iteration 554: 0.685983\n",
      "Cost after iteration 555: 0.685988\n",
      "Cost after iteration 556: 0.685993\n",
      "Cost after iteration 557: 0.685998\n",
      "Cost after iteration 558: 0.686004\n",
      "Cost after iteration 559: 0.686009\n",
      "Cost after iteration 560: 0.686014\n",
      "Cost after iteration 561: 0.686019\n",
      "Cost after iteration 562: 0.686024\n",
      "Cost after iteration 563: 0.686029\n",
      "Cost after iteration 564: 0.686034\n",
      "Cost after iteration 565: 0.686039\n",
      "Cost after iteration 566: 0.686044\n",
      "Cost after iteration 567: 0.686049\n",
      "Cost after iteration 568: 0.686054\n",
      "Cost after iteration 569: 0.686059\n",
      "Cost after iteration 570: 0.686064\n",
      "Cost after iteration 571: 0.686069\n",
      "Cost after iteration 572: 0.686073\n",
      "Cost after iteration 573: 0.686078\n",
      "Cost after iteration 574: 0.686083\n",
      "Cost after iteration 575: 0.686088\n",
      "Cost after iteration 576: 0.686093\n",
      "Cost after iteration 577: 0.686098\n",
      "Cost after iteration 578: 0.686103\n",
      "Cost after iteration 579: 0.686107\n",
      "Cost after iteration 580: 0.686112\n",
      "Cost after iteration 581: 0.686117\n",
      "Cost after iteration 582: 0.686122\n",
      "Cost after iteration 583: 0.686127\n",
      "Cost after iteration 584: 0.686131\n",
      "Cost after iteration 585: 0.686136\n",
      "Cost after iteration 586: 0.686141\n",
      "Cost after iteration 587: 0.686146\n",
      "Cost after iteration 588: 0.686151\n",
      "Cost after iteration 589: 0.686155\n",
      "Cost after iteration 590: 0.686160\n",
      "Cost after iteration 591: 0.686165\n",
      "Cost after iteration 592: 0.686170\n",
      "Cost after iteration 593: 0.686174\n",
      "Cost after iteration 594: 0.686179\n",
      "Cost after iteration 595: 0.686184\n",
      "Cost after iteration 596: 0.686189\n",
      "Cost after iteration 597: 0.686193\n",
      "Cost after iteration 598: 0.686198\n",
      "Cost after iteration 599: 0.686203\n",
      "Cost after iteration 600: 0.686208\n",
      "Cost after iteration 601: 0.686212\n",
      "Cost after iteration 602: 0.686217\n",
      "Cost after iteration 603: 0.686222\n",
      "Cost after iteration 604: 0.686227\n",
      "Cost after iteration 605: 0.686231\n",
      "Cost after iteration 606: 0.686236\n",
      "Cost after iteration 607: 0.686241\n",
      "Cost after iteration 608: 0.686246\n",
      "Cost after iteration 609: 0.686250\n",
      "Cost after iteration 610: 0.686255\n",
      "Cost after iteration 611: 0.686260\n",
      "Cost after iteration 612: 0.686265\n",
      "Cost after iteration 613: 0.686269\n",
      "Cost after iteration 614: 0.686274\n",
      "Cost after iteration 615: 0.686279\n",
      "Cost after iteration 616: 0.686284\n",
      "Cost after iteration 617: 0.686289\n",
      "Cost after iteration 618: 0.686293\n",
      "Cost after iteration 619: 0.686298\n",
      "Cost after iteration 620: 0.686303\n",
      "Cost after iteration 621: 0.686308\n",
      "Cost after iteration 622: 0.686312\n",
      "Cost after iteration 623: 0.686317\n",
      "Cost after iteration 624: 0.686322\n",
      "Cost after iteration 625: 0.686327\n",
      "Cost after iteration 626: 0.686332\n",
      "Cost after iteration 627: 0.686337\n",
      "Cost after iteration 628: 0.686341\n",
      "Cost after iteration 629: 0.686346\n",
      "Cost after iteration 630: 0.686351\n",
      "Cost after iteration 631: 0.686356\n",
      "Cost after iteration 632: 0.686361\n",
      "Cost after iteration 633: 0.686366\n",
      "Cost after iteration 634: 0.686370\n",
      "Cost after iteration 635: 0.686375\n",
      "Cost after iteration 636: 0.686380\n",
      "Cost after iteration 637: 0.686385\n",
      "Cost after iteration 638: 0.686390\n",
      "Cost after iteration 639: 0.686395\n",
      "Cost after iteration 640: 0.686400\n",
      "Cost after iteration 641: 0.686404\n",
      "Cost after iteration 642: 0.686409\n",
      "Cost after iteration 643: 0.686414\n",
      "Cost after iteration 644: 0.686419\n",
      "Cost after iteration 645: 0.686424\n",
      "Cost after iteration 646: 0.686429\n",
      "Cost after iteration 647: 0.686434\n",
      "Cost after iteration 648: 0.686438\n",
      "Cost after iteration 649: 0.686443\n",
      "Cost after iteration 650: 0.686448\n",
      "Cost after iteration 651: 0.686453\n",
      "Cost after iteration 652: 0.686458\n",
      "Cost after iteration 653: 0.686462\n",
      "Cost after iteration 654: 0.686467\n",
      "Cost after iteration 655: 0.686472\n",
      "Cost after iteration 656: 0.686477\n",
      "Cost after iteration 657: 0.686481\n",
      "Cost after iteration 658: 0.686486\n",
      "Cost after iteration 659: 0.686491\n",
      "Cost after iteration 660: 0.686496\n",
      "Cost after iteration 661: 0.686500\n",
      "Cost after iteration 662: 0.686505\n",
      "Cost after iteration 663: 0.686510\n",
      "Cost after iteration 664: 0.686514\n",
      "Cost after iteration 665: 0.686519\n",
      "Cost after iteration 666: 0.686523\n",
      "Cost after iteration 667: 0.686528\n",
      "Cost after iteration 668: 0.686533\n",
      "Cost after iteration 669: 0.686537\n",
      "Cost after iteration 670: 0.686542\n",
      "Cost after iteration 671: 0.686546\n",
      "Cost after iteration 672: 0.686550\n",
      "Cost after iteration 673: 0.686555\n",
      "Cost after iteration 674: 0.686559\n",
      "Cost after iteration 675: 0.686564\n",
      "Cost after iteration 676: 0.686568\n",
      "Cost after iteration 677: 0.686572\n",
      "Cost after iteration 678: 0.686576\n",
      "Cost after iteration 679: 0.686581\n",
      "Cost after iteration 680: 0.686585\n",
      "Cost after iteration 681: 0.686589\n",
      "Cost after iteration 682: 0.686593\n",
      "Cost after iteration 683: 0.686597\n",
      "Cost after iteration 684: 0.686601\n",
      "Cost after iteration 685: 0.686605\n",
      "Cost after iteration 686: 0.686609\n",
      "Cost after iteration 687: 0.686613\n",
      "Cost after iteration 688: 0.686617\n",
      "Cost after iteration 689: 0.686621\n",
      "Cost after iteration 690: 0.686625\n",
      "Cost after iteration 691: 0.686628\n",
      "Cost after iteration 692: 0.686632\n",
      "Cost after iteration 693: 0.686636\n",
      "Cost after iteration 694: 0.686640\n",
      "Cost after iteration 695: 0.686643\n",
      "Cost after iteration 696: 0.686647\n",
      "Cost after iteration 697: 0.686651\n",
      "Cost after iteration 698: 0.686654\n",
      "Cost after iteration 699: 0.686658\n",
      "Cost after iteration 700: 0.686661\n",
      "Cost after iteration 701: 0.686665\n",
      "Cost after iteration 702: 0.686668\n",
      "Cost after iteration 703: 0.686672\n",
      "Cost after iteration 704: 0.686675\n",
      "Cost after iteration 705: 0.686679\n",
      "Cost after iteration 706: 0.686682\n",
      "Cost after iteration 707: 0.686686\n",
      "Cost after iteration 708: 0.686689\n",
      "Cost after iteration 709: 0.686692\n",
      "Cost after iteration 710: 0.686696\n",
      "Cost after iteration 711: 0.686699\n",
      "Cost after iteration 712: 0.686702\n",
      "Cost after iteration 713: 0.686706\n",
      "Cost after iteration 714: 0.686709\n",
      "Cost after iteration 715: 0.686713\n",
      "Cost after iteration 716: 0.686716\n",
      "Cost after iteration 717: 0.686719\n",
      "Cost after iteration 718: 0.686723\n",
      "Cost after iteration 719: 0.686726\n",
      "Cost after iteration 720: 0.686729\n",
      "Cost after iteration 721: 0.686733\n",
      "Cost after iteration 722: 0.686736\n",
      "Cost after iteration 723: 0.686740\n",
      "Cost after iteration 724: 0.686743\n",
      "Cost after iteration 725: 0.686747\n",
      "Cost after iteration 726: 0.686750\n",
      "Cost after iteration 727: 0.686753\n",
      "Cost after iteration 728: 0.686757\n",
      "Cost after iteration 729: 0.686760\n",
      "Cost after iteration 730: 0.686764\n",
      "Cost after iteration 731: 0.686768\n",
      "Cost after iteration 732: 0.686771\n",
      "Cost after iteration 733: 0.686775\n",
      "Cost after iteration 734: 0.686778\n",
      "Cost after iteration 735: 0.686782\n",
      "Cost after iteration 736: 0.686785\n",
      "Cost after iteration 737: 0.686789\n",
      "Cost after iteration 738: 0.686793\n",
      "Cost after iteration 739: 0.686796\n",
      "Cost after iteration 740: 0.686800\n",
      "Cost after iteration 741: 0.686804\n",
      "Cost after iteration 742: 0.686807\n",
      "Cost after iteration 743: 0.686811\n",
      "Cost after iteration 744: 0.686815\n",
      "Cost after iteration 745: 0.686819\n",
      "Cost after iteration 746: 0.686822\n",
      "Cost after iteration 747: 0.686826\n",
      "Cost after iteration 748: 0.686830\n",
      "Cost after iteration 749: 0.686834\n",
      "Cost after iteration 750: 0.686837\n",
      "Cost after iteration 751: 0.686841\n",
      "Cost after iteration 752: 0.686845\n",
      "Cost after iteration 753: 0.686849\n",
      "Cost after iteration 754: 0.686852\n",
      "Cost after iteration 755: 0.686856\n",
      "Cost after iteration 756: 0.686860\n",
      "Cost after iteration 757: 0.686863\n",
      "Cost after iteration 758: 0.686867\n",
      "Cost after iteration 759: 0.686871\n",
      "Cost after iteration 760: 0.686874\n",
      "Cost after iteration 761: 0.686878\n",
      "Cost after iteration 762: 0.686882\n",
      "Cost after iteration 763: 0.686885\n",
      "Cost after iteration 764: 0.686889\n",
      "Cost after iteration 765: 0.686892\n",
      "Cost after iteration 766: 0.686896\n",
      "Cost after iteration 767: 0.686899\n",
      "Cost after iteration 768: 0.686903\n",
      "Cost after iteration 769: 0.686906\n",
      "Cost after iteration 770: 0.686910\n",
      "Cost after iteration 771: 0.686913\n",
      "Cost after iteration 772: 0.686917\n",
      "Cost after iteration 773: 0.686920\n",
      "Cost after iteration 774: 0.686923\n",
      "Cost after iteration 775: 0.686926\n",
      "Cost after iteration 776: 0.686930\n",
      "Cost after iteration 777: 0.686933\n",
      "Cost after iteration 778: 0.686936\n",
      "Cost after iteration 779: 0.686939\n",
      "Cost after iteration 780: 0.686942\n",
      "Cost after iteration 781: 0.686945\n",
      "Cost after iteration 782: 0.686949\n",
      "Cost after iteration 783: 0.686952\n",
      "Cost after iteration 784: 0.686955\n",
      "Cost after iteration 785: 0.686957\n",
      "Cost after iteration 786: 0.686960\n",
      "Cost after iteration 787: 0.686963\n",
      "Cost after iteration 788: 0.686966\n",
      "Cost after iteration 789: 0.686969\n",
      "Cost after iteration 790: 0.686972\n",
      "Cost after iteration 791: 0.686975\n",
      "Cost after iteration 792: 0.686977\n",
      "Cost after iteration 793: 0.686980\n",
      "Cost after iteration 794: 0.686983\n",
      "Cost after iteration 795: 0.686985\n",
      "Cost after iteration 796: 0.686988\n",
      "Cost after iteration 797: 0.686990\n",
      "Cost after iteration 798: 0.686993\n",
      "Cost after iteration 799: 0.686996\n",
      "Cost after iteration 800: 0.686998\n",
      "Cost after iteration 801: 0.687001\n",
      "Cost after iteration 802: 0.687003\n",
      "Cost after iteration 803: 0.687005\n",
      "Cost after iteration 804: 0.687008\n",
      "Cost after iteration 805: 0.687010\n",
      "Cost after iteration 806: 0.687012\n",
      "Cost after iteration 807: 0.687015\n",
      "Cost after iteration 808: 0.687017\n",
      "Cost after iteration 809: 0.687019\n",
      "Cost after iteration 810: 0.687021\n",
      "Cost after iteration 811: 0.687024\n",
      "Cost after iteration 812: 0.687026\n",
      "Cost after iteration 813: 0.687028\n",
      "Cost after iteration 814: 0.687030\n",
      "Cost after iteration 815: 0.687032\n",
      "Cost after iteration 816: 0.687034\n",
      "Cost after iteration 817: 0.687036\n",
      "Cost after iteration 818: 0.687038\n",
      "Cost after iteration 819: 0.687040\n",
      "Cost after iteration 820: 0.687042\n",
      "Cost after iteration 821: 0.687044\n",
      "Cost after iteration 822: 0.687046\n",
      "Cost after iteration 823: 0.687048\n",
      "Cost after iteration 824: 0.687050\n",
      "Cost after iteration 825: 0.687052\n",
      "Cost after iteration 826: 0.687054\n",
      "Cost after iteration 827: 0.687056\n",
      "Cost after iteration 828: 0.687057\n",
      "Cost after iteration 829: 0.687059\n",
      "Cost after iteration 830: 0.687061\n",
      "Cost after iteration 831: 0.687063\n",
      "Cost after iteration 832: 0.687064\n",
      "Cost after iteration 833: 0.687066\n",
      "Cost after iteration 834: 0.687068\n",
      "Cost after iteration 835: 0.687070\n",
      "Cost after iteration 836: 0.687071\n",
      "Cost after iteration 837: 0.687073\n",
      "Cost after iteration 838: 0.687075\n",
      "Cost after iteration 839: 0.687076\n",
      "Cost after iteration 840: 0.687078\n",
      "Cost after iteration 841: 0.687079\n",
      "Cost after iteration 842: 0.687081\n",
      "Cost after iteration 843: 0.687083\n",
      "Cost after iteration 844: 0.687084\n",
      "Cost after iteration 845: 0.687086\n",
      "Cost after iteration 846: 0.687087\n",
      "Cost after iteration 847: 0.687089\n",
      "Cost after iteration 848: 0.687090\n",
      "Cost after iteration 849: 0.687092\n",
      "Cost after iteration 850: 0.687093\n",
      "Cost after iteration 851: 0.687095\n",
      "Cost after iteration 852: 0.687096\n",
      "Cost after iteration 853: 0.687098\n",
      "Cost after iteration 854: 0.687099\n",
      "Cost after iteration 855: 0.687100\n",
      "Cost after iteration 856: 0.687102\n",
      "Cost after iteration 857: 0.687103\n",
      "Cost after iteration 858: 0.687105\n",
      "Cost after iteration 859: 0.687106\n",
      "Cost after iteration 860: 0.687107\n",
      "Cost after iteration 861: 0.687109\n",
      "Cost after iteration 862: 0.687110\n",
      "Cost after iteration 863: 0.687111\n",
      "Cost after iteration 864: 0.687113\n",
      "Cost after iteration 865: 0.687114\n",
      "Cost after iteration 866: 0.687115\n",
      "Cost after iteration 867: 0.687116\n",
      "Cost after iteration 868: 0.687118\n",
      "Cost after iteration 869: 0.687119\n",
      "Cost after iteration 870: 0.687120\n",
      "Cost after iteration 871: 0.687121\n",
      "Cost after iteration 872: 0.687123\n",
      "Cost after iteration 873: 0.687124\n",
      "Cost after iteration 874: 0.687125\n",
      "Cost after iteration 875: 0.687126\n",
      "Cost after iteration 876: 0.687128\n",
      "Cost after iteration 877: 0.687129\n",
      "Cost after iteration 878: 0.687130\n",
      "Cost after iteration 879: 0.687131\n",
      "Cost after iteration 880: 0.687132\n",
      "Cost after iteration 881: 0.687133\n",
      "Cost after iteration 882: 0.687135\n",
      "Cost after iteration 883: 0.687136\n",
      "Cost after iteration 884: 0.687137\n",
      "Cost after iteration 885: 0.687138\n",
      "Cost after iteration 886: 0.687139\n",
      "Cost after iteration 887: 0.687140\n",
      "Cost after iteration 888: 0.687141\n",
      "Cost after iteration 889: 0.687142\n",
      "Cost after iteration 890: 0.687144\n",
      "Cost after iteration 891: 0.687145\n",
      "Cost after iteration 892: 0.687146\n",
      "Cost after iteration 893: 0.687147\n",
      "Cost after iteration 894: 0.687148\n",
      "Cost after iteration 895: 0.687149\n",
      "Cost after iteration 896: 0.687150\n",
      "Cost after iteration 897: 0.687151\n",
      "Cost after iteration 898: 0.687152\n",
      "Cost after iteration 899: 0.687153\n",
      "Cost after iteration 900: 0.687154\n",
      "Cost after iteration 901: 0.687155\n",
      "Cost after iteration 902: 0.687156\n",
      "Cost after iteration 903: 0.687157\n",
      "Cost after iteration 904: 0.687158\n",
      "Cost after iteration 905: 0.687159\n",
      "Cost after iteration 906: 0.687160\n",
      "Cost after iteration 907: 0.687161\n",
      "Cost after iteration 908: 0.687162\n",
      "Cost after iteration 909: 0.687163\n",
      "Cost after iteration 910: 0.687164\n",
      "Cost after iteration 911: 0.687165\n",
      "Cost after iteration 912: 0.687166\n",
      "Cost after iteration 913: 0.687167\n",
      "Cost after iteration 914: 0.687168\n",
      "Cost after iteration 915: 0.687169\n",
      "Cost after iteration 916: 0.687170\n",
      "Cost after iteration 917: 0.687171\n",
      "Cost after iteration 918: 0.687172\n",
      "Cost after iteration 919: 0.687173\n",
      "Cost after iteration 920: 0.687174\n",
      "Cost after iteration 921: 0.687175\n",
      "Cost after iteration 922: 0.687176\n",
      "Cost after iteration 923: 0.687176\n",
      "Cost after iteration 924: 0.687177\n",
      "Cost after iteration 925: 0.687178\n",
      "Cost after iteration 926: 0.687179\n",
      "Cost after iteration 927: 0.687180\n",
      "Cost after iteration 928: 0.687181\n",
      "Cost after iteration 929: 0.687182\n",
      "Cost after iteration 930: 0.687183\n",
      "Cost after iteration 931: 0.687184\n",
      "Cost after iteration 932: 0.687185\n",
      "Cost after iteration 933: 0.687185\n",
      "Cost after iteration 934: 0.687186\n",
      "Cost after iteration 935: 0.687187\n",
      "Cost after iteration 936: 0.687188\n",
      "Cost after iteration 937: 0.687189\n",
      "Cost after iteration 938: 0.687190\n",
      "Cost after iteration 939: 0.687191\n",
      "Cost after iteration 940: 0.687192\n",
      "Cost after iteration 941: 0.687192\n",
      "Cost after iteration 942: 0.687193\n",
      "Cost after iteration 943: 0.687194\n",
      "Cost after iteration 944: 0.687195\n",
      "Cost after iteration 945: 0.687196\n",
      "Cost after iteration 946: 0.687197\n",
      "Cost after iteration 947: 0.687198\n",
      "Cost after iteration 948: 0.687198\n",
      "Cost after iteration 949: 0.687199\n",
      "Cost after iteration 950: 0.687200\n",
      "Cost after iteration 951: 0.687201\n",
      "Cost after iteration 952: 0.687202\n",
      "Cost after iteration 953: 0.687202\n",
      "Cost after iteration 954: 0.687203\n",
      "Cost after iteration 955: 0.687204\n",
      "Cost after iteration 956: 0.687205\n",
      "Cost after iteration 957: 0.687206\n",
      "Cost after iteration 958: 0.687207\n",
      "Cost after iteration 959: 0.687207\n",
      "Cost after iteration 960: 0.687208\n",
      "Cost after iteration 961: 0.687209\n",
      "Cost after iteration 962: 0.687210\n",
      "Cost after iteration 963: 0.687211\n",
      "Cost after iteration 964: 0.687211\n",
      "Cost after iteration 965: 0.687212\n",
      "Cost after iteration 966: 0.687213\n",
      "Cost after iteration 967: 0.687214\n",
      "Cost after iteration 968: 0.687215\n",
      "Cost after iteration 969: 0.687215\n",
      "Cost after iteration 970: 0.687216\n",
      "Cost after iteration 971: 0.687217\n",
      "Cost after iteration 972: 0.687218\n",
      "Cost after iteration 973: 0.687218\n",
      "Cost after iteration 974: 0.687219\n",
      "Cost after iteration 975: 0.687220\n",
      "Cost after iteration 976: 0.687221\n",
      "Cost after iteration 977: 0.687222\n",
      "Cost after iteration 978: 0.687222\n",
      "Cost after iteration 979: 0.687223\n",
      "Cost after iteration 980: 0.687224\n",
      "Cost after iteration 981: 0.687225\n",
      "Cost after iteration 982: 0.687225\n",
      "Cost after iteration 983: 0.687226\n",
      "Cost after iteration 984: 0.687227\n",
      "Cost after iteration 985: 0.687228\n",
      "Cost after iteration 986: 0.687228\n",
      "Cost after iteration 987: 0.687229\n",
      "Cost after iteration 988: 0.687230\n",
      "Cost after iteration 989: 0.687231\n",
      "Cost after iteration 990: 0.687231\n",
      "Cost after iteration 991: 0.687232\n",
      "Cost after iteration 992: 0.687233\n",
      "Cost after iteration 993: 0.687234\n",
      "Cost after iteration 994: 0.687234\n",
      "Cost after iteration 995: 0.687235\n",
      "Cost after iteration 996: 0.687236\n",
      "Cost after iteration 997: 0.687237\n",
      "Cost after iteration 998: 0.687237\n",
      "Cost after iteration 999: 0.687238\n",
      "Cost after iteration 1000: 0.687239\n",
      "Cost after iteration 1001: 0.687239\n",
      "Cost after iteration 1002: 0.687240\n",
      "Cost after iteration 1003: 0.687241\n",
      "Cost after iteration 1004: 0.687242\n",
      "Cost after iteration 1005: 0.687242\n",
      "Cost after iteration 1006: 0.687243\n",
      "Cost after iteration 1007: 0.687244\n",
      "Cost after iteration 1008: 0.687245\n",
      "Cost after iteration 1009: 0.687245\n",
      "Cost after iteration 1010: 0.687246\n",
      "Cost after iteration 1011: 0.687247\n",
      "Cost after iteration 1012: 0.687247\n",
      "Cost after iteration 1013: 0.687248\n",
      "Cost after iteration 1014: 0.687249\n",
      "Cost after iteration 1015: 0.687250\n",
      "Cost after iteration 1016: 0.687250\n",
      "Cost after iteration 1017: 0.687251\n",
      "Cost after iteration 1018: 0.687252\n",
      "Cost after iteration 1019: 0.687252\n",
      "Cost after iteration 1020: 0.687253\n",
      "Cost after iteration 1021: 0.687254\n",
      "Cost after iteration 1022: 0.687254\n",
      "Cost after iteration 1023: 0.687255\n",
      "Cost after iteration 1024: 0.687256\n",
      "Cost after iteration 1025: 0.687257\n",
      "Cost after iteration 1026: 0.687257\n",
      "Cost after iteration 1027: 0.687258\n",
      "Cost after iteration 1028: 0.687259\n",
      "Cost after iteration 1029: 0.687259\n",
      "Cost after iteration 1030: 0.687260\n",
      "Cost after iteration 1031: 0.687261\n",
      "Cost after iteration 1032: 0.687261\n",
      "Cost after iteration 1033: 0.687262\n",
      "Cost after iteration 1034: 0.687263\n",
      "Cost after iteration 1035: 0.687264\n",
      "Cost after iteration 1036: 0.687264\n",
      "Cost after iteration 1037: 0.687265\n",
      "Cost after iteration 1038: 0.687266\n",
      "Cost after iteration 1039: 0.687266\n",
      "Cost after iteration 1040: 0.687267\n",
      "Cost after iteration 1041: 0.687268\n",
      "Cost after iteration 1042: 0.687268\n",
      "Cost after iteration 1043: 0.687269\n",
      "Cost after iteration 1044: 0.687270\n",
      "Cost after iteration 1045: 0.687270\n",
      "Cost after iteration 1046: 0.687271\n",
      "Cost after iteration 1047: 0.687272\n",
      "Cost after iteration 1048: 0.687272\n",
      "Cost after iteration 1049: 0.687273\n",
      "Cost after iteration 1050: 0.687274\n",
      "Cost after iteration 1051: 0.687275\n",
      "Cost after iteration 1052: 0.687275\n",
      "Cost after iteration 1053: 0.687276\n",
      "Cost after iteration 1054: 0.687277\n",
      "Cost after iteration 1055: 0.687277\n",
      "Cost after iteration 1056: 0.687278\n",
      "Cost after iteration 1057: 0.687279\n",
      "Cost after iteration 1058: 0.687279\n",
      "Cost after iteration 1059: 0.687280\n",
      "Cost after iteration 1060: 0.687281\n",
      "Cost after iteration 1061: 0.687281\n",
      "Cost after iteration 1062: 0.687282\n",
      "Cost after iteration 1063: 0.687283\n",
      "Cost after iteration 1064: 0.687283\n",
      "Cost after iteration 1065: 0.687284\n",
      "Cost after iteration 1066: 0.687285\n",
      "Cost after iteration 1067: 0.687285\n",
      "Cost after iteration 1068: 0.687286\n",
      "Cost after iteration 1069: 0.687287\n",
      "Cost after iteration 1070: 0.687287\n",
      "Cost after iteration 1071: 0.687288\n",
      "Cost after iteration 1072: 0.687289\n",
      "Cost after iteration 1073: 0.687289\n",
      "Cost after iteration 1074: 0.687290\n",
      "Cost after iteration 1075: 0.687291\n",
      "Cost after iteration 1076: 0.687291\n",
      "Cost after iteration 1077: 0.687292\n",
      "Cost after iteration 1078: 0.687293\n",
      "Cost after iteration 1079: 0.687293\n",
      "Cost after iteration 1080: 0.687294\n",
      "Cost after iteration 1081: 0.687295\n",
      "Cost after iteration 1082: 0.687295\n",
      "Cost after iteration 1083: 0.687296\n",
      "Cost after iteration 1084: 0.687297\n",
      "Cost after iteration 1085: 0.687297\n",
      "Cost after iteration 1086: 0.687298\n",
      "Cost after iteration 1087: 0.687299\n",
      "Cost after iteration 1088: 0.687299\n",
      "Cost after iteration 1089: 0.687300\n",
      "Cost after iteration 1090: 0.687301\n",
      "Cost after iteration 1091: 0.687301\n",
      "Cost after iteration 1092: 0.687302\n",
      "Cost after iteration 1093: 0.687303\n",
      "Cost after iteration 1094: 0.687303\n",
      "Cost after iteration 1095: 0.687304\n",
      "Cost after iteration 1096: 0.687305\n",
      "Cost after iteration 1097: 0.687305\n",
      "Cost after iteration 1098: 0.687306\n",
      "Cost after iteration 1099: 0.687307\n",
      "Cost after iteration 1100: 0.687307\n",
      "Cost after iteration 1101: 0.687308\n",
      "Cost after iteration 1102: 0.687309\n",
      "Cost after iteration 1103: 0.687309\n",
      "Cost after iteration 1104: 0.687310\n",
      "Cost after iteration 1105: 0.687311\n",
      "Cost after iteration 1106: 0.687311\n",
      "Cost after iteration 1107: 0.687312\n",
      "Cost after iteration 1108: 0.687313\n",
      "Cost after iteration 1109: 0.687313\n",
      "Cost after iteration 1110: 0.687314\n",
      "Cost after iteration 1111: 0.687315\n",
      "Cost after iteration 1112: 0.687315\n",
      "Cost after iteration 1113: 0.687316\n",
      "Cost after iteration 1114: 0.687317\n",
      "Cost after iteration 1115: 0.687317\n",
      "Cost after iteration 1116: 0.687318\n",
      "Cost after iteration 1117: 0.687318\n",
      "Cost after iteration 1118: 0.687319\n",
      "Cost after iteration 1119: 0.687320\n",
      "Cost after iteration 1120: 0.687320\n",
      "Cost after iteration 1121: 0.687321\n",
      "Cost after iteration 1122: 0.687322\n",
      "Cost after iteration 1123: 0.687322\n",
      "Cost after iteration 1124: 0.687323\n",
      "Cost after iteration 1125: 0.687324\n",
      "Cost after iteration 1126: 0.687324\n",
      "Cost after iteration 1127: 0.687325\n",
      "Cost after iteration 1128: 0.687326\n",
      "Cost after iteration 1129: 0.687326\n",
      "Cost after iteration 1130: 0.687327\n",
      "Cost after iteration 1131: 0.687328\n",
      "Cost after iteration 1132: 0.687328\n",
      "Cost after iteration 1133: 0.687329\n",
      "Cost after iteration 1134: 0.687330\n",
      "Cost after iteration 1135: 0.687330\n",
      "Cost after iteration 1136: 0.687331\n",
      "Cost after iteration 1137: 0.687332\n",
      "Cost after iteration 1138: 0.687332\n",
      "Cost after iteration 1139: 0.687333\n",
      "Cost after iteration 1140: 0.687334\n",
      "Cost after iteration 1141: 0.687334\n",
      "Cost after iteration 1142: 0.687335\n",
      "Cost after iteration 1143: 0.687336\n",
      "Cost after iteration 1144: 0.687336\n",
      "Cost after iteration 1145: 0.687337\n",
      "Cost after iteration 1146: 0.687338\n",
      "Cost after iteration 1147: 0.687338\n",
      "Cost after iteration 1148: 0.687339\n",
      "Cost after iteration 1149: 0.687340\n",
      "Cost after iteration 1150: 0.687340\n",
      "Cost after iteration 1151: 0.687341\n",
      "Cost after iteration 1152: 0.687342\n",
      "Cost after iteration 1153: 0.687342\n",
      "Cost after iteration 1154: 0.687343\n",
      "Cost after iteration 1155: 0.687344\n",
      "Cost after iteration 1156: 0.687344\n",
      "Cost after iteration 1157: 0.687345\n",
      "Cost after iteration 1158: 0.687346\n",
      "Cost after iteration 1159: 0.687346\n",
      "Cost after iteration 1160: 0.687347\n",
      "Cost after iteration 1161: 0.687348\n",
      "Cost after iteration 1162: 0.687348\n",
      "Cost after iteration 1163: 0.687349\n",
      "Cost after iteration 1164: 0.687350\n",
      "Cost after iteration 1165: 0.687350\n",
      "Cost after iteration 1166: 0.687351\n",
      "Cost after iteration 1167: 0.687352\n",
      "Cost after iteration 1168: 0.687352\n",
      "Cost after iteration 1169: 0.687353\n",
      "Cost after iteration 1170: 0.687354\n",
      "Cost after iteration 1171: 0.687354\n",
      "Cost after iteration 1172: 0.687355\n",
      "Cost after iteration 1173: 0.687356\n",
      "Cost after iteration 1174: 0.687356\n",
      "Cost after iteration 1175: 0.687357\n",
      "Cost after iteration 1176: 0.687358\n",
      "Cost after iteration 1177: 0.687358\n",
      "Cost after iteration 1178: 0.687359\n",
      "Cost after iteration 1179: 0.687360\n",
      "Cost after iteration 1180: 0.687360\n",
      "Cost after iteration 1181: 0.687361\n",
      "Cost after iteration 1182: 0.687362\n",
      "Cost after iteration 1183: 0.687362\n",
      "Cost after iteration 1184: 0.687363\n",
      "Cost after iteration 1185: 0.687364\n",
      "Cost after iteration 1186: 0.687364\n",
      "Cost after iteration 1187: 0.687365\n",
      "Cost after iteration 1188: 0.687366\n",
      "Cost after iteration 1189: 0.687367\n",
      "Cost after iteration 1190: 0.687367\n",
      "Cost after iteration 1191: 0.687368\n",
      "Cost after iteration 1192: 0.687369\n",
      "Cost after iteration 1193: 0.687369\n",
      "Cost after iteration 1194: 0.687370\n",
      "Cost after iteration 1195: 0.687371\n",
      "Cost after iteration 1196: 0.687371\n",
      "Cost after iteration 1197: 0.687372\n",
      "Cost after iteration 1198: 0.687373\n",
      "Cost after iteration 1199: 0.687373\n",
      "Cost after iteration 1200: 0.687374\n",
      "Cost after iteration 1201: 0.687375\n",
      "Cost after iteration 1202: 0.687375\n",
      "Cost after iteration 1203: 0.687376\n",
      "Cost after iteration 1204: 0.687377\n",
      "Cost after iteration 1205: 0.687377\n",
      "Cost after iteration 1206: 0.687378\n",
      "Cost after iteration 1207: 0.687379\n",
      "Cost after iteration 1208: 0.687380\n",
      "Cost after iteration 1209: 0.687380\n",
      "Cost after iteration 1210: 0.687381\n",
      "Cost after iteration 1211: 0.687382\n",
      "Cost after iteration 1212: 0.687382\n",
      "Cost after iteration 1213: 0.687383\n",
      "Cost after iteration 1214: 0.687384\n",
      "Cost after iteration 1215: 0.687384\n",
      "Cost after iteration 1216: 0.687385\n",
      "Cost after iteration 1217: 0.687386\n",
      "Cost after iteration 1218: 0.687387\n",
      "Cost after iteration 1219: 0.687387\n",
      "Cost after iteration 1220: 0.687388\n",
      "Cost after iteration 1221: 0.687389\n",
      "Cost after iteration 1222: 0.687389\n",
      "Cost after iteration 1223: 0.687390\n",
      "Cost after iteration 1224: 0.687391\n",
      "Cost after iteration 1225: 0.687391\n",
      "Cost after iteration 1226: 0.687392\n",
      "Cost after iteration 1227: 0.687393\n",
      "Cost after iteration 1228: 0.687394\n",
      "Cost after iteration 1229: 0.687394\n",
      "Cost after iteration 1230: 0.687395\n",
      "Cost after iteration 1231: 0.687396\n",
      "Cost after iteration 1232: 0.687396\n",
      "Cost after iteration 1233: 0.687397\n",
      "Cost after iteration 1234: 0.687398\n",
      "Cost after iteration 1235: 0.687398\n",
      "Cost after iteration 1236: 0.687399\n",
      "Cost after iteration 1237: 0.687400\n",
      "Cost after iteration 1238: 0.687401\n",
      "Cost after iteration 1239: 0.687401\n",
      "Cost after iteration 1240: 0.687402\n",
      "Cost after iteration 1241: 0.687403\n",
      "Cost after iteration 1242: 0.687403\n",
      "Cost after iteration 1243: 0.687404\n",
      "Cost after iteration 1244: 0.687405\n",
      "Cost after iteration 1245: 0.687406\n",
      "Cost after iteration 1246: 0.687406\n",
      "Cost after iteration 1247: 0.687407\n",
      "Cost after iteration 1248: 0.687408\n",
      "Cost after iteration 1249: 0.687409\n",
      "Cost after iteration 1250: 0.687409\n",
      "Cost after iteration 1251: 0.687410\n",
      "Cost after iteration 1252: 0.687411\n",
      "Cost after iteration 1253: 0.687411\n",
      "Cost after iteration 1254: 0.687412\n",
      "Cost after iteration 1255: 0.687413\n",
      "Cost after iteration 1256: 0.687414\n",
      "Cost after iteration 1257: 0.687414\n",
      "Cost after iteration 1258: 0.687415\n",
      "Cost after iteration 1259: 0.687416\n",
      "Cost after iteration 1260: 0.687416\n",
      "Cost after iteration 1261: 0.687417\n",
      "Cost after iteration 1262: 0.687418\n",
      "Cost after iteration 1263: 0.687419\n",
      "Cost after iteration 1264: 0.687419\n",
      "Cost after iteration 1265: 0.687420\n",
      "Cost after iteration 1266: 0.687421\n",
      "Cost after iteration 1267: 0.687422\n",
      "Cost after iteration 1268: 0.687422\n",
      "Cost after iteration 1269: 0.687423\n",
      "Cost after iteration 1270: 0.687424\n",
      "Cost after iteration 1271: 0.687425\n",
      "Cost after iteration 1272: 0.687425\n",
      "Cost after iteration 1273: 0.687426\n",
      "Cost after iteration 1274: 0.687427\n",
      "Cost after iteration 1275: 0.687428\n",
      "Cost after iteration 1276: 0.687428\n",
      "Cost after iteration 1277: 0.687429\n",
      "Cost after iteration 1278: 0.687430\n",
      "Cost after iteration 1279: 0.687431\n",
      "Cost after iteration 1280: 0.687431\n",
      "Cost after iteration 1281: 0.687432\n",
      "Cost after iteration 1282: 0.687433\n",
      "Cost after iteration 1283: 0.687434\n",
      "Cost after iteration 1284: 0.687434\n",
      "Cost after iteration 1285: 0.687435\n",
      "Cost after iteration 1286: 0.687436\n",
      "Cost after iteration 1287: 0.687437\n",
      "Cost after iteration 1288: 0.687437\n",
      "Cost after iteration 1289: 0.687438\n",
      "Cost after iteration 1290: 0.687439\n",
      "Cost after iteration 1291: 0.687440\n",
      "Cost after iteration 1292: 0.687440\n",
      "Cost after iteration 1293: 0.687441\n",
      "Cost after iteration 1294: 0.687442\n",
      "Cost after iteration 1295: 0.687443\n",
      "Cost after iteration 1296: 0.687443\n",
      "Cost after iteration 1297: 0.687444\n",
      "Cost after iteration 1298: 0.687445\n",
      "Cost after iteration 1299: 0.687446\n",
      "Cost after iteration 1300: 0.687446\n",
      "Cost after iteration 1301: 0.687447\n",
      "Cost after iteration 1302: 0.687448\n",
      "Cost after iteration 1303: 0.687449\n",
      "Cost after iteration 1304: 0.687450\n",
      "Cost after iteration 1305: 0.687450\n",
      "Cost after iteration 1306: 0.687451\n",
      "Cost after iteration 1307: 0.687452\n",
      "Cost after iteration 1308: 0.687453\n",
      "Cost after iteration 1309: 0.687453\n",
      "Cost after iteration 1310: 0.687454\n",
      "Cost after iteration 1311: 0.687455\n",
      "Cost after iteration 1312: 0.687456\n",
      "Cost after iteration 1313: 0.687456\n",
      "Cost after iteration 1314: 0.687457\n",
      "Cost after iteration 1315: 0.687458\n",
      "Cost after iteration 1316: 0.687459\n",
      "Cost after iteration 1317: 0.687460\n",
      "Cost after iteration 1318: 0.687460\n",
      "Cost after iteration 1319: 0.687461\n",
      "Cost after iteration 1320: 0.687462\n",
      "Cost after iteration 1321: 0.687463\n",
      "Cost after iteration 1322: 0.687464\n",
      "Cost after iteration 1323: 0.687464\n",
      "Cost after iteration 1324: 0.687465\n",
      "Cost after iteration 1325: 0.687466\n",
      "Cost after iteration 1326: 0.687467\n",
      "Cost after iteration 1327: 0.687468\n",
      "Cost after iteration 1328: 0.687468\n",
      "Cost after iteration 1329: 0.687469\n",
      "Cost after iteration 1330: 0.687470\n",
      "Cost after iteration 1331: 0.687471\n",
      "Cost after iteration 1332: 0.687471\n",
      "Cost after iteration 1333: 0.687472\n",
      "Cost after iteration 1334: 0.687473\n",
      "Cost after iteration 1335: 0.687474\n",
      "Cost after iteration 1336: 0.687475\n",
      "Cost after iteration 1337: 0.687475\n",
      "Cost after iteration 1338: 0.687476\n",
      "Cost after iteration 1339: 0.687477\n",
      "Cost after iteration 1340: 0.687478\n",
      "Cost after iteration 1341: 0.687479\n",
      "Cost after iteration 1342: 0.687480\n",
      "Cost after iteration 1343: 0.687480\n",
      "Cost after iteration 1344: 0.687481\n",
      "Cost after iteration 1345: 0.687482\n",
      "Cost after iteration 1346: 0.687483\n",
      "Cost after iteration 1347: 0.687484\n",
      "Cost after iteration 1348: 0.687484\n",
      "Cost after iteration 1349: 0.687485\n",
      "Cost after iteration 1350: 0.687486\n",
      "Cost after iteration 1351: 0.687487\n",
      "Cost after iteration 1352: 0.687488\n",
      "Cost after iteration 1353: 0.687488\n",
      "Cost after iteration 1354: 0.687489\n",
      "Cost after iteration 1355: 0.687490\n",
      "Cost after iteration 1356: 0.687491\n",
      "Cost after iteration 1357: 0.687492\n",
      "Cost after iteration 1358: 0.687493\n",
      "Cost after iteration 1359: 0.687493\n",
      "Cost after iteration 1360: 0.687494\n",
      "Cost after iteration 1361: 0.687495\n",
      "Cost after iteration 1362: 0.687496\n",
      "Cost after iteration 1363: 0.687497\n",
      "Cost after iteration 1364: 0.687498\n",
      "Cost after iteration 1365: 0.687498\n",
      "Cost after iteration 1366: 0.687499\n",
      "Cost after iteration 1367: 0.687500\n",
      "Cost after iteration 1368: 0.687501\n",
      "Cost after iteration 1369: 0.687502\n",
      "Cost after iteration 1370: 0.687503\n",
      "Cost after iteration 1371: 0.687503\n",
      "Cost after iteration 1372: 0.687504\n",
      "Cost after iteration 1373: 0.687505\n",
      "Cost after iteration 1374: 0.687506\n",
      "Cost after iteration 1375: 0.687507\n",
      "Cost after iteration 1376: 0.687508\n",
      "Cost after iteration 1377: 0.687508\n",
      "Cost after iteration 1378: 0.687509\n",
      "Cost after iteration 1379: 0.687510\n",
      "Cost after iteration 1380: 0.687511\n",
      "Cost after iteration 1381: 0.687512\n",
      "Cost after iteration 1382: 0.687513\n",
      "Cost after iteration 1383: 0.687514\n",
      "Cost after iteration 1384: 0.687514\n",
      "Cost after iteration 1385: 0.687515\n",
      "Cost after iteration 1386: 0.687516\n",
      "Cost after iteration 1387: 0.687517\n",
      "Cost after iteration 1388: 0.687518\n",
      "Cost after iteration 1389: 0.687519\n",
      "Cost after iteration 1390: 0.687520\n",
      "Cost after iteration 1391: 0.687520\n",
      "Cost after iteration 1392: 0.687521\n",
      "Cost after iteration 1393: 0.687522\n",
      "Cost after iteration 1394: 0.687523\n",
      "Cost after iteration 1395: 0.687524\n",
      "Cost after iteration 1396: 0.687525\n",
      "Cost after iteration 1397: 0.687526\n",
      "Cost after iteration 1398: 0.687526\n",
      "Cost after iteration 1399: 0.687527\n",
      "Cost after iteration 1400: 0.687528\n",
      "Cost after iteration 1401: 0.687529\n",
      "Cost after iteration 1402: 0.687530\n",
      "Cost after iteration 1403: 0.687531\n",
      "Cost after iteration 1404: 0.687532\n",
      "Cost after iteration 1405: 0.687533\n",
      "Cost after iteration 1406: 0.687533\n",
      "Cost after iteration 1407: 0.687534\n",
      "Cost after iteration 1408: 0.687535\n",
      "Cost after iteration 1409: 0.687536\n",
      "Cost after iteration 1410: 0.687537\n",
      "Cost after iteration 1411: 0.687538\n",
      "Cost after iteration 1412: 0.687539\n",
      "Cost after iteration 1413: 0.687540\n",
      "Cost after iteration 1414: 0.687541\n",
      "Cost after iteration 1415: 0.687541\n",
      "Cost after iteration 1416: 0.687542\n",
      "Cost after iteration 1417: 0.687543\n",
      "Cost after iteration 1418: 0.687544\n",
      "Cost after iteration 1419: 0.687545\n",
      "Cost after iteration 1420: 0.687546\n",
      "Cost after iteration 1421: 0.687547\n",
      "Cost after iteration 1422: 0.687548\n",
      "Cost after iteration 1423: 0.687549\n",
      "Cost after iteration 1424: 0.687549\n",
      "Cost after iteration 1425: 0.687550\n",
      "Cost after iteration 1426: 0.687551\n",
      "Cost after iteration 1427: 0.687552\n",
      "Cost after iteration 1428: 0.687553\n",
      "Cost after iteration 1429: 0.687554\n",
      "Cost after iteration 1430: 0.687555\n",
      "Cost after iteration 1431: 0.687556\n",
      "Cost after iteration 1432: 0.687557\n",
      "Cost after iteration 1433: 0.687558\n",
      "Cost after iteration 1434: 0.687559\n",
      "Cost after iteration 1435: 0.687559\n",
      "Cost after iteration 1436: 0.687560\n",
      "Cost after iteration 1437: 0.687561\n",
      "Cost after iteration 1438: 0.687562\n",
      "Cost after iteration 1439: 0.687563\n",
      "Cost after iteration 1440: 0.687564\n",
      "Cost after iteration 1441: 0.687565\n",
      "Cost after iteration 1442: 0.687566\n",
      "Cost after iteration 1443: 0.687567\n",
      "Cost after iteration 1444: 0.687568\n",
      "Cost after iteration 1445: 0.687569\n",
      "Cost after iteration 1446: 0.687570\n",
      "Cost after iteration 1447: 0.687571\n",
      "Cost after iteration 1448: 0.687571\n",
      "Cost after iteration 1449: 0.687572\n",
      "Cost after iteration 1450: 0.687573\n",
      "Cost after iteration 1451: 0.687574\n",
      "Cost after iteration 1452: 0.687575\n",
      "Cost after iteration 1453: 0.687576\n",
      "Cost after iteration 1454: 0.687577\n",
      "Cost after iteration 1455: 0.687578\n",
      "Cost after iteration 1456: 0.687579\n",
      "Cost after iteration 1457: 0.687580\n",
      "Cost after iteration 1458: 0.687581\n",
      "Cost after iteration 1459: 0.687582\n",
      "Cost after iteration 1460: 0.687583\n",
      "Cost after iteration 1461: 0.687584\n",
      "Cost after iteration 1462: 0.687585\n",
      "Cost after iteration 1463: 0.687586\n",
      "Cost after iteration 1464: 0.687587\n",
      "Cost after iteration 1465: 0.687588\n",
      "Cost after iteration 1466: 0.687588\n",
      "Cost after iteration 1467: 0.687589\n",
      "Cost after iteration 1468: 0.687590\n",
      "Cost after iteration 1469: 0.687591\n",
      "Cost after iteration 1470: 0.687592\n",
      "Cost after iteration 1471: 0.687593\n",
      "Cost after iteration 1472: 0.687594\n",
      "Cost after iteration 1473: 0.687595\n",
      "Cost after iteration 1474: 0.687596\n",
      "Cost after iteration 1475: 0.687597\n",
      "Cost after iteration 1476: 0.687598\n",
      "Cost after iteration 1477: 0.687599\n",
      "Cost after iteration 1478: 0.687600\n",
      "Cost after iteration 1479: 0.687601\n",
      "Cost after iteration 1480: 0.687602\n",
      "Cost after iteration 1481: 0.687603\n",
      "Cost after iteration 1482: 0.687604\n",
      "Cost after iteration 1483: 0.687605\n",
      "Cost after iteration 1484: 0.687606\n",
      "Cost after iteration 1485: 0.687607\n",
      "Cost after iteration 1486: 0.687608\n",
      "Cost after iteration 1487: 0.687609\n",
      "Cost after iteration 1488: 0.687610\n",
      "Cost after iteration 1489: 0.687611\n",
      "Cost after iteration 1490: 0.687612\n",
      "Cost after iteration 1491: 0.687613\n",
      "Cost after iteration 1492: 0.687614\n",
      "Cost after iteration 1493: 0.687615\n",
      "Cost after iteration 1494: 0.687616\n",
      "Cost after iteration 1495: 0.687617\n",
      "Cost after iteration 1496: 0.687618\n",
      "Cost after iteration 1497: 0.687619\n",
      "Cost after iteration 1498: 0.687620\n",
      "Cost after iteration 1499: 0.687621\n",
      "Cost after iteration 1500: 0.687622\n",
      "Cost after iteration 1501: 0.687623\n",
      "Cost after iteration 1502: 0.687624\n",
      "Cost after iteration 1503: 0.687625\n",
      "Cost after iteration 1504: 0.687626\n",
      "Cost after iteration 1505: 0.687627\n",
      "Cost after iteration 1506: 0.687628\n",
      "Cost after iteration 1507: 0.687629\n",
      "Cost after iteration 1508: 0.687630\n",
      "Cost after iteration 1509: 0.687631\n",
      "Cost after iteration 1510: 0.687632\n",
      "Cost after iteration 1511: 0.687633\n",
      "Cost after iteration 1512: 0.687634\n",
      "Cost after iteration 1513: 0.687635\n",
      "Cost after iteration 1514: 0.687636\n",
      "Cost after iteration 1515: 0.687637\n",
      "Cost after iteration 1516: 0.687638\n",
      "Cost after iteration 1517: 0.687640\n",
      "Cost after iteration 1518: 0.687641\n",
      "Cost after iteration 1519: 0.687642\n",
      "Cost after iteration 1520: 0.687643\n",
      "Cost after iteration 1521: 0.687644\n",
      "Cost after iteration 1522: 0.687645\n",
      "Cost after iteration 1523: 0.687646\n",
      "Cost after iteration 1524: 0.687647\n",
      "Cost after iteration 1525: 0.687648\n",
      "Cost after iteration 1526: 0.687649\n",
      "Cost after iteration 1527: 0.687650\n",
      "Cost after iteration 1528: 0.687651\n",
      "Cost after iteration 1529: 0.687652\n",
      "Cost after iteration 1530: 0.687653\n",
      "Cost after iteration 1531: 0.687654\n",
      "Cost after iteration 1532: 0.687655\n",
      "Cost after iteration 1533: 0.687656\n",
      "Cost after iteration 1534: 0.687658\n",
      "Cost after iteration 1535: 0.687659\n",
      "Cost after iteration 1536: 0.687660\n",
      "Cost after iteration 1537: 0.687661\n",
      "Cost after iteration 1538: 0.687662\n",
      "Cost after iteration 1539: 0.687663\n",
      "Cost after iteration 1540: 0.687664\n",
      "Cost after iteration 1541: 0.687665\n",
      "Cost after iteration 1542: 0.687666\n",
      "Cost after iteration 1543: 0.687667\n",
      "Cost after iteration 1544: 0.687668\n",
      "Cost after iteration 1545: 0.687670\n",
      "Cost after iteration 1546: 0.687671\n",
      "Cost after iteration 1547: 0.687672\n",
      "Cost after iteration 1548: 0.687673\n",
      "Cost after iteration 1549: 0.687674\n",
      "Cost after iteration 1550: 0.687675\n",
      "Cost after iteration 1551: 0.687676\n",
      "Cost after iteration 1552: 0.687677\n",
      "Cost after iteration 1553: 0.687678\n",
      "Cost after iteration 1554: 0.687679\n",
      "Cost after iteration 1555: 0.687681\n",
      "Cost after iteration 1556: 0.687682\n",
      "Cost after iteration 1557: 0.687683\n",
      "Cost after iteration 1558: 0.687684\n",
      "Cost after iteration 1559: 0.687685\n",
      "Cost after iteration 1560: 0.687686\n",
      "Cost after iteration 1561: 0.687687\n",
      "Cost after iteration 1562: 0.687688\n",
      "Cost after iteration 1563: 0.687690\n",
      "Cost after iteration 1564: 0.687691\n",
      "Cost after iteration 1565: 0.687692\n",
      "Cost after iteration 1566: 0.687693\n",
      "Cost after iteration 1567: 0.687694\n",
      "Cost after iteration 1568: 0.687695\n",
      "Cost after iteration 1569: 0.687696\n",
      "Cost after iteration 1570: 0.687698\n",
      "Cost after iteration 1571: 0.687699\n",
      "Cost after iteration 1572: 0.687700\n",
      "Cost after iteration 1573: 0.687701\n",
      "Cost after iteration 1574: 0.687702\n",
      "Cost after iteration 1575: 0.687703\n",
      "Cost after iteration 1576: 0.687704\n",
      "Cost after iteration 1577: 0.687706\n",
      "Cost after iteration 1578: 0.687707\n",
      "Cost after iteration 1579: 0.687708\n",
      "Cost after iteration 1580: 0.687709\n",
      "Cost after iteration 1581: 0.687710\n",
      "Cost after iteration 1582: 0.687711\n",
      "Cost after iteration 1583: 0.687713\n",
      "Cost after iteration 1584: 0.687714\n",
      "Cost after iteration 1585: 0.687715\n",
      "Cost after iteration 1586: 0.687716\n",
      "Cost after iteration 1587: 0.687717\n",
      "Cost after iteration 1588: 0.687719\n",
      "Cost after iteration 1589: 0.687720\n",
      "Cost after iteration 1590: 0.687721\n",
      "Cost after iteration 1591: 0.687722\n",
      "Cost after iteration 1592: 0.687723\n",
      "Cost after iteration 1593: 0.687724\n",
      "Cost after iteration 1594: 0.687726\n",
      "Cost after iteration 1595: 0.687727\n",
      "Cost after iteration 1596: 0.687728\n",
      "Cost after iteration 1597: 0.687729\n",
      "Cost after iteration 1598: 0.687730\n",
      "Cost after iteration 1599: 0.687732\n",
      "Cost after iteration 1600: 0.687733\n",
      "Cost after iteration 1601: 0.687734\n",
      "Cost after iteration 1602: 0.687735\n",
      "Cost after iteration 1603: 0.687736\n",
      "Cost after iteration 1604: 0.687738\n",
      "Cost after iteration 1605: 0.687739\n",
      "Cost after iteration 1606: 0.687740\n",
      "Cost after iteration 1607: 0.687741\n",
      "Cost after iteration 1608: 0.687743\n",
      "Cost after iteration 1609: 0.687744\n",
      "Cost after iteration 1610: 0.687745\n",
      "Cost after iteration 1611: 0.687746\n",
      "Cost after iteration 1612: 0.687747\n",
      "Cost after iteration 1613: 0.687749\n",
      "Cost after iteration 1614: 0.687750\n",
      "Cost after iteration 1615: 0.687751\n",
      "Cost after iteration 1616: 0.687752\n",
      "Cost after iteration 1617: 0.687754\n",
      "Cost after iteration 1618: 0.687755\n",
      "Cost after iteration 1619: 0.687756\n",
      "Cost after iteration 1620: 0.687757\n",
      "Cost after iteration 1621: 0.687758\n",
      "Cost after iteration 1622: 0.687760\n",
      "Cost after iteration 1623: 0.687761\n",
      "Cost after iteration 1624: 0.687762\n",
      "Cost after iteration 1625: 0.687763\n",
      "Cost after iteration 1626: 0.687765\n",
      "Cost after iteration 1627: 0.687766\n",
      "Cost after iteration 1628: 0.687767\n",
      "Cost after iteration 1629: 0.687768\n",
      "Cost after iteration 1630: 0.687770\n",
      "Cost after iteration 1631: 0.687771\n",
      "Cost after iteration 1632: 0.687772\n",
      "Cost after iteration 1633: 0.687773\n",
      "Cost after iteration 1634: 0.687775\n",
      "Cost after iteration 1635: 0.687776\n",
      "Cost after iteration 1636: 0.687777\n",
      "Cost after iteration 1637: 0.687779\n",
      "Cost after iteration 1638: 0.687780\n",
      "Cost after iteration 1639: 0.687781\n",
      "Cost after iteration 1640: 0.687782\n",
      "Cost after iteration 1641: 0.687784\n",
      "Cost after iteration 1642: 0.687785\n",
      "Cost after iteration 1643: 0.687786\n",
      "Cost after iteration 1644: 0.687787\n",
      "Cost after iteration 1645: 0.687789\n",
      "Cost after iteration 1646: 0.687790\n",
      "Cost after iteration 1647: 0.687791\n",
      "Cost after iteration 1648: 0.687792\n",
      "Cost after iteration 1649: 0.687794\n",
      "Cost after iteration 1650: 0.687795\n",
      "Cost after iteration 1651: 0.687796\n",
      "Cost after iteration 1652: 0.687798\n",
      "Cost after iteration 1653: 0.687799\n",
      "Cost after iteration 1654: 0.687800\n",
      "Cost after iteration 1655: 0.687801\n",
      "Cost after iteration 1656: 0.687803\n",
      "Cost after iteration 1657: 0.687804\n",
      "Cost after iteration 1658: 0.687805\n",
      "Cost after iteration 1659: 0.687807\n",
      "Cost after iteration 1660: 0.687808\n",
      "Cost after iteration 1661: 0.687809\n",
      "Cost after iteration 1662: 0.687810\n",
      "Cost after iteration 1663: 0.687812\n",
      "Cost after iteration 1664: 0.687813\n",
      "Cost after iteration 1665: 0.687814\n",
      "Cost after iteration 1666: 0.687816\n",
      "Cost after iteration 1667: 0.687817\n",
      "Cost after iteration 1668: 0.687818\n",
      "Cost after iteration 1669: 0.687819\n",
      "Cost after iteration 1670: 0.687821\n",
      "Cost after iteration 1671: 0.687822\n",
      "Cost after iteration 1672: 0.687823\n",
      "Cost after iteration 1673: 0.687825\n",
      "Cost after iteration 1674: 0.687826\n",
      "Cost after iteration 1675: 0.687827\n",
      "Cost after iteration 1676: 0.687829\n",
      "Cost after iteration 1677: 0.687830\n",
      "Cost after iteration 1678: 0.687831\n",
      "Cost after iteration 1679: 0.687832\n",
      "Cost after iteration 1680: 0.687834\n",
      "Cost after iteration 1681: 0.687835\n",
      "Cost after iteration 1682: 0.687836\n",
      "Cost after iteration 1683: 0.687838\n",
      "Cost after iteration 1684: 0.687839\n",
      "Cost after iteration 1685: 0.687840\n",
      "Cost after iteration 1686: 0.687842\n",
      "Cost after iteration 1687: 0.687843\n",
      "Cost after iteration 1688: 0.687844\n",
      "Cost after iteration 1689: 0.687845\n",
      "Cost after iteration 1690: 0.687847\n",
      "Cost after iteration 1691: 0.687848\n",
      "Cost after iteration 1692: 0.687849\n",
      "Cost after iteration 1693: 0.687851\n",
      "Cost after iteration 1694: 0.687852\n",
      "Cost after iteration 1695: 0.687853\n",
      "Cost after iteration 1696: 0.687855\n",
      "Cost after iteration 1697: 0.687856\n",
      "Cost after iteration 1698: 0.687857\n",
      "Cost after iteration 1699: 0.687858\n",
      "Cost after iteration 1700: 0.687860\n",
      "Cost after iteration 1701: 0.687861\n",
      "Cost after iteration 1702: 0.687862\n",
      "Cost after iteration 1703: 0.687864\n",
      "Cost after iteration 1704: 0.687865\n",
      "Cost after iteration 1705: 0.687866\n",
      "Cost after iteration 1706: 0.687868\n",
      "Cost after iteration 1707: 0.687869\n",
      "Cost after iteration 1708: 0.687870\n",
      "Cost after iteration 1709: 0.687871\n",
      "Cost after iteration 1710: 0.687873\n",
      "Cost after iteration 1711: 0.687874\n",
      "Cost after iteration 1712: 0.687875\n",
      "Cost after iteration 1713: 0.687877\n",
      "Cost after iteration 1714: 0.687878\n",
      "Cost after iteration 1715: 0.687879\n",
      "Cost after iteration 1716: 0.687881\n",
      "Cost after iteration 1717: 0.687882\n",
      "Cost after iteration 1718: 0.687883\n",
      "Cost after iteration 1719: 0.687884\n",
      "Cost after iteration 1720: 0.687886\n",
      "Cost after iteration 1721: 0.687887\n",
      "Cost after iteration 1722: 0.687888\n",
      "Cost after iteration 1723: 0.687890\n",
      "Cost after iteration 1724: 0.687891\n",
      "Cost after iteration 1725: 0.687892\n",
      "Cost after iteration 1726: 0.687893\n",
      "Cost after iteration 1727: 0.687895\n",
      "Cost after iteration 1728: 0.687896\n",
      "Cost after iteration 1729: 0.687897\n",
      "Cost after iteration 1730: 0.687899\n",
      "Cost after iteration 1731: 0.687900\n",
      "Cost after iteration 1732: 0.687901\n",
      "Cost after iteration 1733: 0.687902\n",
      "Cost after iteration 1734: 0.687904\n",
      "Cost after iteration 1735: 0.687905\n",
      "Cost after iteration 1736: 0.687906\n",
      "Cost after iteration 1737: 0.687908\n",
      "Cost after iteration 1738: 0.687909\n",
      "Cost after iteration 1739: 0.687910\n",
      "Cost after iteration 1740: 0.687911\n",
      "Cost after iteration 1741: 0.687913\n",
      "Cost after iteration 1742: 0.687914\n",
      "Cost after iteration 1743: 0.687915\n",
      "Cost after iteration 1744: 0.687917\n",
      "Cost after iteration 1745: 0.687918\n",
      "Cost after iteration 1746: 0.687919\n",
      "Cost after iteration 1747: 0.687920\n",
      "Cost after iteration 1748: 0.687922\n",
      "Cost after iteration 1749: 0.687923\n",
      "Cost after iteration 1750: 0.687924\n",
      "Cost after iteration 1751: 0.687925\n",
      "Cost after iteration 1752: 0.687927\n",
      "Cost after iteration 1753: 0.687928\n",
      "Cost after iteration 1754: 0.687929\n",
      "Cost after iteration 1755: 0.687931\n",
      "Cost after iteration 1756: 0.687932\n",
      "Cost after iteration 1757: 0.687933\n",
      "Cost after iteration 1758: 0.687934\n",
      "Cost after iteration 1759: 0.687936\n",
      "Cost after iteration 1760: 0.687937\n",
      "Cost after iteration 1761: 0.687938\n",
      "Cost after iteration 1762: 0.687939\n",
      "Cost after iteration 1763: 0.687941\n",
      "Cost after iteration 1764: 0.687942\n",
      "Cost after iteration 1765: 0.687943\n",
      "Cost after iteration 1766: 0.687944\n",
      "Cost after iteration 1767: 0.687946\n",
      "Cost after iteration 1768: 0.687947\n",
      "Cost after iteration 1769: 0.687948\n",
      "Cost after iteration 1770: 0.687949\n",
      "Cost after iteration 1771: 0.687951\n",
      "Cost after iteration 1772: 0.687952\n",
      "Cost after iteration 1773: 0.687953\n",
      "Cost after iteration 1774: 0.687954\n",
      "Cost after iteration 1775: 0.687956\n",
      "Cost after iteration 1776: 0.687957\n",
      "Cost after iteration 1777: 0.687958\n",
      "Cost after iteration 1778: 0.687959\n",
      "Cost after iteration 1779: 0.687960\n",
      "Cost after iteration 1780: 0.687962\n",
      "Cost after iteration 1781: 0.687963\n",
      "Cost after iteration 1782: 0.687964\n",
      "Cost after iteration 1783: 0.687965\n",
      "Cost after iteration 1784: 0.687967\n",
      "Cost after iteration 1785: 0.687968\n",
      "Cost after iteration 1786: 0.687969\n",
      "Cost after iteration 1787: 0.687970\n",
      "Cost after iteration 1788: 0.687971\n",
      "Cost after iteration 1789: 0.687973\n",
      "Cost after iteration 1790: 0.687974\n",
      "Cost after iteration 1791: 0.687975\n",
      "Cost after iteration 1792: 0.687976\n",
      "Cost after iteration 1793: 0.687978\n",
      "Cost after iteration 1794: 0.687979\n",
      "Cost after iteration 1795: 0.687980\n",
      "Cost after iteration 1796: 0.687981\n",
      "Cost after iteration 1797: 0.687982\n",
      "Cost after iteration 1798: 0.687984\n",
      "Cost after iteration 1799: 0.687985\n",
      "Cost after iteration 1800: 0.687986\n",
      "Cost after iteration 1801: 0.687987\n",
      "Cost after iteration 1802: 0.687988\n",
      "Cost after iteration 1803: 0.687989\n",
      "Cost after iteration 1804: 0.687991\n",
      "Cost after iteration 1805: 0.687992\n",
      "Cost after iteration 1806: 0.687993\n",
      "Cost after iteration 1807: 0.687994\n",
      "Cost after iteration 1808: 0.687995\n",
      "Cost after iteration 1809: 0.687997\n",
      "Cost after iteration 1810: 0.687998\n",
      "Cost after iteration 1811: 0.687999\n",
      "Cost after iteration 1812: 0.688000\n",
      "Cost after iteration 1813: 0.688001\n",
      "Cost after iteration 1814: 0.688002\n",
      "Cost after iteration 1815: 0.688004\n",
      "Cost after iteration 1816: 0.688005\n",
      "Cost after iteration 1817: 0.688006\n",
      "Cost after iteration 1818: 0.688007\n",
      "Cost after iteration 1819: 0.688008\n",
      "Cost after iteration 1820: 0.688009\n",
      "Cost after iteration 1821: 0.688011\n",
      "Cost after iteration 1822: 0.688012\n",
      "Cost after iteration 1823: 0.688013\n",
      "Cost after iteration 1824: 0.688014\n",
      "Cost after iteration 1825: 0.688015\n",
      "Cost after iteration 1826: 0.688016\n",
      "Cost after iteration 1827: 0.688017\n",
      "Cost after iteration 1828: 0.688019\n",
      "Cost after iteration 1829: 0.688020\n",
      "Cost after iteration 1830: 0.688021\n",
      "Cost after iteration 1831: 0.688022\n",
      "Cost after iteration 1832: 0.688023\n",
      "Cost after iteration 1833: 0.688024\n",
      "Cost after iteration 1834: 0.688025\n",
      "Cost after iteration 1835: 0.688027\n",
      "Cost after iteration 1836: 0.688028\n",
      "Cost after iteration 1837: 0.688029\n",
      "Cost after iteration 1838: 0.688030\n",
      "Cost after iteration 1839: 0.688031\n",
      "Cost after iteration 1840: 0.688032\n",
      "Cost after iteration 1841: 0.688033\n",
      "Cost after iteration 1842: 0.688034\n",
      "Cost after iteration 1843: 0.688036\n",
      "Cost after iteration 1844: 0.688037\n",
      "Cost after iteration 1845: 0.688038\n",
      "Cost after iteration 1846: 0.688039\n",
      "Cost after iteration 1847: 0.688040\n",
      "Cost after iteration 1848: 0.688041\n",
      "Cost after iteration 1849: 0.688042\n",
      "Cost after iteration 1850: 0.688043\n",
      "Cost after iteration 1851: 0.688044\n",
      "Cost after iteration 1852: 0.688045\n",
      "Cost after iteration 1853: 0.688047\n",
      "Cost after iteration 1854: 0.688048\n",
      "Cost after iteration 1855: 0.688049\n",
      "Cost after iteration 1856: 0.688050\n",
      "Cost after iteration 1857: 0.688051\n",
      "Cost after iteration 1858: 0.688052\n",
      "Cost after iteration 1859: 0.688053\n",
      "Cost after iteration 1860: 0.688054\n",
      "Cost after iteration 1861: 0.688055\n",
      "Cost after iteration 1862: 0.688056\n",
      "Cost after iteration 1863: 0.688057\n",
      "Cost after iteration 1864: 0.688059\n",
      "Cost after iteration 1865: 0.688060\n",
      "Cost after iteration 1866: 0.688061\n",
      "Cost after iteration 1867: 0.688062\n",
      "Cost after iteration 1868: 0.688063\n",
      "Cost after iteration 1869: 0.688064\n",
      "Cost after iteration 1870: 0.688065\n",
      "Cost after iteration 1871: 0.688066\n",
      "Cost after iteration 1872: 0.688067\n",
      "Cost after iteration 1873: 0.688068\n",
      "Cost after iteration 1874: 0.688069\n",
      "Cost after iteration 1875: 0.688070\n",
      "Cost after iteration 1876: 0.688071\n",
      "Cost after iteration 1877: 0.688072\n",
      "Cost after iteration 1878: 0.688073\n",
      "Cost after iteration 1879: 0.688075\n",
      "Cost after iteration 1880: 0.688076\n",
      "Cost after iteration 1881: 0.688077\n",
      "Cost after iteration 1882: 0.688078\n",
      "Cost after iteration 1883: 0.688079\n",
      "Cost after iteration 1884: 0.688080\n",
      "Cost after iteration 1885: 0.688081\n",
      "Cost after iteration 1886: 0.688082\n",
      "Cost after iteration 1887: 0.688083\n",
      "Cost after iteration 1888: 0.688084\n",
      "Cost after iteration 1889: 0.688085\n",
      "Cost after iteration 1890: 0.688086\n",
      "Cost after iteration 1891: 0.688087\n",
      "Cost after iteration 1892: 0.688088\n",
      "Cost after iteration 1893: 0.688089\n",
      "Cost after iteration 1894: 0.688090\n",
      "Cost after iteration 1895: 0.688091\n",
      "Cost after iteration 1896: 0.688092\n",
      "Cost after iteration 1897: 0.688093\n",
      "Cost after iteration 1898: 0.688094\n",
      "Cost after iteration 1899: 0.688095\n",
      "Cost after iteration 1900: 0.688096\n",
      "Cost after iteration 1901: 0.688097\n",
      "Cost after iteration 1902: 0.688098\n",
      "Cost after iteration 1903: 0.688100\n",
      "Cost after iteration 1904: 0.688101\n",
      "Cost after iteration 1905: 0.688102\n",
      "Cost after iteration 1906: 0.688103\n",
      "Cost after iteration 1907: 0.688104\n",
      "Cost after iteration 1908: 0.688105\n",
      "Cost after iteration 1909: 0.688106\n",
      "Cost after iteration 1910: 0.688107\n",
      "Cost after iteration 1911: 0.688108\n",
      "Cost after iteration 1912: 0.688109\n",
      "Cost after iteration 1913: 0.688110\n",
      "Cost after iteration 1914: 0.688111\n",
      "Cost after iteration 1915: 0.688112\n",
      "Cost after iteration 1916: 0.688113\n",
      "Cost after iteration 1917: 0.688114\n",
      "Cost after iteration 1918: 0.688115\n",
      "Cost after iteration 1919: 0.688116\n",
      "Cost after iteration 1920: 0.688117\n",
      "Cost after iteration 1921: 0.688118\n",
      "Cost after iteration 1922: 0.688119\n",
      "Cost after iteration 1923: 0.688120\n",
      "Cost after iteration 1924: 0.688121\n",
      "Cost after iteration 1925: 0.688122\n",
      "Cost after iteration 1926: 0.688123\n",
      "Cost after iteration 1927: 0.688124\n",
      "Cost after iteration 1928: 0.688125\n",
      "Cost after iteration 1929: 0.688126\n",
      "Cost after iteration 1930: 0.688127\n",
      "Cost after iteration 1931: 0.688128\n",
      "Cost after iteration 1932: 0.688129\n",
      "Cost after iteration 1933: 0.688130\n",
      "Cost after iteration 1934: 0.688131\n",
      "Cost after iteration 1935: 0.688132\n",
      "Cost after iteration 1936: 0.688133\n",
      "Cost after iteration 1937: 0.688134\n",
      "Cost after iteration 1938: 0.688135\n",
      "Cost after iteration 1939: 0.688136\n",
      "Cost after iteration 1940: 0.688137\n",
      "Cost after iteration 1941: 0.688138\n",
      "Cost after iteration 1942: 0.688139\n",
      "Cost after iteration 1943: 0.688140\n",
      "Cost after iteration 1944: 0.688141\n",
      "Cost after iteration 1945: 0.688142\n",
      "Cost after iteration 1946: 0.688143\n",
      "Cost after iteration 1947: 0.688144\n",
      "Cost after iteration 1948: 0.688145\n",
      "Cost after iteration 1949: 0.688146\n",
      "Cost after iteration 1950: 0.688147\n",
      "Cost after iteration 1951: 0.688147\n",
      "Cost after iteration 1952: 0.688148\n",
      "Cost after iteration 1953: 0.688149\n",
      "Cost after iteration 1954: 0.688150\n",
      "Cost after iteration 1955: 0.688151\n",
      "Cost after iteration 1956: 0.688152\n",
      "Cost after iteration 1957: 0.688153\n",
      "Cost after iteration 1958: 0.688154\n",
      "Cost after iteration 1959: 0.688155\n",
      "Cost after iteration 1960: 0.688156\n",
      "Cost after iteration 1961: 0.688157\n",
      "Cost after iteration 1962: 0.688158\n",
      "Cost after iteration 1963: 0.688159\n",
      "Cost after iteration 1964: 0.688160\n",
      "Cost after iteration 1965: 0.688161\n",
      "Cost after iteration 1966: 0.688162\n",
      "Cost after iteration 1967: 0.688163\n",
      "Cost after iteration 1968: 0.688164\n",
      "Cost after iteration 1969: 0.688165\n",
      "Cost after iteration 1970: 0.688166\n",
      "Cost after iteration 1971: 0.688167\n",
      "Cost after iteration 1972: 0.688168\n",
      "Cost after iteration 1973: 0.688169\n",
      "Cost after iteration 1974: 0.688170\n",
      "Cost after iteration 1975: 0.688171\n",
      "Cost after iteration 1976: 0.688172\n",
      "Cost after iteration 1977: 0.688173\n",
      "Cost after iteration 1978: 0.688174\n",
      "Cost after iteration 1979: 0.688175\n",
      "Cost after iteration 1980: 0.688176\n",
      "Cost after iteration 1981: 0.688177\n",
      "Cost after iteration 1982: 0.688177\n",
      "Cost after iteration 1983: 0.688178\n",
      "Cost after iteration 1984: 0.688179\n",
      "Cost after iteration 1985: 0.688180\n",
      "Cost after iteration 1986: 0.688181\n",
      "Cost after iteration 1987: 0.688182\n",
      "Cost after iteration 1988: 0.688183\n",
      "Cost after iteration 1989: 0.688184\n",
      "Cost after iteration 1990: 0.688185\n",
      "Cost after iteration 1991: 0.688186\n",
      "Cost after iteration 1992: 0.688187\n",
      "Cost after iteration 1993: 0.688188\n",
      "Cost after iteration 1994: 0.688189\n",
      "Cost after iteration 1995: 0.688190\n",
      "Cost after iteration 1996: 0.688191\n",
      "Cost after iteration 1997: 0.688192\n",
      "Cost after iteration 1998: 0.688193\n",
      "Cost after iteration 1999: 0.688194\n",
      "Cost after iteration 2000: 0.688195\n",
      "Cost after iteration 2001: 0.688196\n",
      "Cost after iteration 2002: 0.688197\n",
      "Cost after iteration 2003: 0.688197\n",
      "Cost after iteration 2004: 0.688198\n",
      "Cost after iteration 2005: 0.688199\n",
      "Cost after iteration 2006: 0.688200\n",
      "Cost after iteration 2007: 0.688201\n",
      "Cost after iteration 2008: 0.688202\n",
      "Cost after iteration 2009: 0.688203\n",
      "Cost after iteration 2010: 0.688204\n",
      "Cost after iteration 2011: 0.688205\n",
      "Cost after iteration 2012: 0.688206\n",
      "Cost after iteration 2013: 0.688207\n",
      "Cost after iteration 2014: 0.688208\n",
      "Cost after iteration 2015: 0.688209\n",
      "Cost after iteration 2016: 0.688210\n",
      "Cost after iteration 2017: 0.688211\n",
      "Cost after iteration 2018: 0.688212\n",
      "Cost after iteration 2019: 0.688213\n",
      "Cost after iteration 2020: 0.688213\n",
      "Cost after iteration 2021: 0.688214\n",
      "Cost after iteration 2022: 0.688215\n",
      "Cost after iteration 2023: 0.688216\n",
      "Cost after iteration 2024: 0.688217\n",
      "Cost after iteration 2025: 0.688218\n",
      "Cost after iteration 2026: 0.688219\n",
      "Cost after iteration 2027: 0.688220\n",
      "Cost after iteration 2028: 0.688221\n",
      "Cost after iteration 2029: 0.688222\n",
      "Cost after iteration 2030: 0.688223\n",
      "Cost after iteration 2031: 0.688224\n",
      "Cost after iteration 2032: 0.688225\n",
      "Cost after iteration 2033: 0.688226\n",
      "Cost after iteration 2034: 0.688227\n",
      "Cost after iteration 2035: 0.688227\n",
      "Cost after iteration 2036: 0.688228\n",
      "Cost after iteration 2037: 0.688229\n",
      "Cost after iteration 2038: 0.688230\n",
      "Cost after iteration 2039: 0.688231\n",
      "Cost after iteration 2040: 0.688232\n",
      "Cost after iteration 2041: 0.688233\n",
      "Cost after iteration 2042: 0.688234\n",
      "Cost after iteration 2043: 0.688235\n",
      "Cost after iteration 2044: 0.688236\n",
      "Cost after iteration 2045: 0.688237\n",
      "Cost after iteration 2046: 0.688238\n",
      "Cost after iteration 2047: 0.688239\n",
      "Cost after iteration 2048: 0.688239\n",
      "Cost after iteration 2049: 0.688240\n",
      "Cost after iteration 2050: 0.688241\n",
      "Cost after iteration 2051: 0.688242\n",
      "Cost after iteration 2052: 0.688243\n",
      "Cost after iteration 2053: 0.688244\n",
      "Cost after iteration 2054: 0.688245\n",
      "Cost after iteration 2055: 0.688246\n",
      "Cost after iteration 2056: 0.688247\n",
      "Cost after iteration 2057: 0.688248\n",
      "Cost after iteration 2058: 0.688249\n",
      "Cost after iteration 2059: 0.688250\n",
      "Cost after iteration 2060: 0.688251\n",
      "Cost after iteration 2061: 0.688251\n",
      "Cost after iteration 2062: 0.688252\n",
      "Cost after iteration 2063: 0.688253\n",
      "Cost after iteration 2064: 0.688254\n",
      "Cost after iteration 2065: 0.688255\n",
      "Cost after iteration 2066: 0.688256\n",
      "Cost after iteration 2067: 0.688257\n",
      "Cost after iteration 2068: 0.688258\n",
      "Cost after iteration 2069: 0.688259\n",
      "Cost after iteration 2070: 0.688260\n",
      "Cost after iteration 2071: 0.688261\n",
      "Cost after iteration 2072: 0.688262\n",
      "Cost after iteration 2073: 0.688262\n",
      "Cost after iteration 2074: 0.688263\n",
      "Cost after iteration 2075: 0.688264\n",
      "Cost after iteration 2076: 0.688265\n",
      "Cost after iteration 2077: 0.688266\n",
      "Cost after iteration 2078: 0.688267\n",
      "Cost after iteration 2079: 0.688268\n",
      "Cost after iteration 2080: 0.688269\n",
      "Cost after iteration 2081: 0.688270\n",
      "Cost after iteration 2082: 0.688271\n",
      "Cost after iteration 2083: 0.688272\n",
      "Cost after iteration 2084: 0.688272\n",
      "Cost after iteration 2085: 0.688273\n",
      "Cost after iteration 2086: 0.688274\n",
      "Cost after iteration 2087: 0.688275\n",
      "Cost after iteration 2088: 0.688276\n",
      "Cost after iteration 2089: 0.688277\n",
      "Cost after iteration 2090: 0.688278\n",
      "Cost after iteration 2091: 0.688279\n",
      "Cost after iteration 2092: 0.688280\n",
      "Cost after iteration 2093: 0.688281\n",
      "Cost after iteration 2094: 0.688282\n",
      "Cost after iteration 2095: 0.688282\n",
      "Cost after iteration 2096: 0.688283\n",
      "Cost after iteration 2097: 0.688284\n",
      "Cost after iteration 2098: 0.688285\n",
      "Cost after iteration 2099: 0.688286\n",
      "Cost after iteration 2100: 0.688287\n",
      "Cost after iteration 2101: 0.688288\n",
      "Cost after iteration 2102: 0.688289\n",
      "Cost after iteration 2103: 0.688290\n",
      "Cost after iteration 2104: 0.688291\n",
      "Cost after iteration 2105: 0.688291\n",
      "Cost after iteration 2106: 0.688292\n",
      "Cost after iteration 2107: 0.688293\n",
      "Cost after iteration 2108: 0.688294\n",
      "Cost after iteration 2109: 0.688295\n",
      "Cost after iteration 2110: 0.688296\n",
      "Cost after iteration 2111: 0.688297\n",
      "Cost after iteration 2112: 0.688298\n",
      "Cost after iteration 2113: 0.688299\n",
      "Cost after iteration 2114: 0.688299\n",
      "Cost after iteration 2115: 0.688300\n",
      "Cost after iteration 2116: 0.688301\n",
      "Cost after iteration 2117: 0.688302\n",
      "Cost after iteration 2118: 0.688303\n",
      "Cost after iteration 2119: 0.688304\n",
      "Cost after iteration 2120: 0.688305\n",
      "Cost after iteration 2121: 0.688306\n",
      "Cost after iteration 2122: 0.688307\n",
      "Cost after iteration 2123: 0.688307\n",
      "Cost after iteration 2124: 0.688308\n",
      "Cost after iteration 2125: 0.688309\n",
      "Cost after iteration 2126: 0.688310\n",
      "Cost after iteration 2127: 0.688311\n",
      "Cost after iteration 2128: 0.688312\n",
      "Cost after iteration 2129: 0.688313\n",
      "Cost after iteration 2130: 0.688314\n",
      "Cost after iteration 2131: 0.688315\n",
      "Cost after iteration 2132: 0.688315\n",
      "Cost after iteration 2133: 0.688316\n",
      "Cost after iteration 2134: 0.688317\n",
      "Cost after iteration 2135: 0.688318\n",
      "Cost after iteration 2136: 0.688319\n",
      "Cost after iteration 2137: 0.688320\n",
      "Cost after iteration 2138: 0.688321\n",
      "Cost after iteration 2139: 0.688322\n",
      "Cost after iteration 2140: 0.688323\n",
      "Cost after iteration 2141: 0.688323\n",
      "Cost after iteration 2142: 0.688324\n",
      "Cost after iteration 2143: 0.688325\n",
      "Cost after iteration 2144: 0.688326\n",
      "Cost after iteration 2145: 0.688327\n",
      "Cost after iteration 2146: 0.688328\n",
      "Cost after iteration 2147: 0.688329\n",
      "Cost after iteration 2148: 0.688330\n",
      "Cost after iteration 2149: 0.688330\n",
      "Cost after iteration 2150: 0.688331\n",
      "Cost after iteration 2151: 0.688332\n",
      "Cost after iteration 2152: 0.688333\n",
      "Cost after iteration 2153: 0.688334\n",
      "Cost after iteration 2154: 0.688335\n",
      "Cost after iteration 2155: 0.688336\n",
      "Cost after iteration 2156: 0.688336\n",
      "Cost after iteration 2157: 0.688337\n",
      "Cost after iteration 2158: 0.688338\n",
      "Cost after iteration 2159: 0.688339\n",
      "Cost after iteration 2160: 0.688340\n",
      "Cost after iteration 2161: 0.688341\n",
      "Cost after iteration 2162: 0.688342\n",
      "Cost after iteration 2163: 0.688343\n",
      "Cost after iteration 2164: 0.688343\n",
      "Cost after iteration 2165: 0.688344\n",
      "Cost after iteration 2166: 0.688345\n",
      "Cost after iteration 2167: 0.688346\n",
      "Cost after iteration 2168: 0.688347\n",
      "Cost after iteration 2169: 0.688348\n",
      "Cost after iteration 2170: 0.688349\n",
      "Cost after iteration 2171: 0.688349\n",
      "Cost after iteration 2172: 0.688350\n",
      "Cost after iteration 2173: 0.688351\n",
      "Cost after iteration 2174: 0.688352\n",
      "Cost after iteration 2175: 0.688353\n",
      "Cost after iteration 2176: 0.688354\n",
      "Cost after iteration 2177: 0.688355\n",
      "Cost after iteration 2178: 0.688355\n",
      "Cost after iteration 2179: 0.688356\n",
      "Cost after iteration 2180: 0.688357\n",
      "Cost after iteration 2181: 0.688358\n",
      "Cost after iteration 2182: 0.688359\n",
      "Cost after iteration 2183: 0.688360\n",
      "Cost after iteration 2184: 0.688361\n",
      "Cost after iteration 2185: 0.688361\n",
      "Cost after iteration 2186: 0.688362\n",
      "Cost after iteration 2187: 0.688363\n",
      "Cost after iteration 2188: 0.688364\n",
      "Cost after iteration 2189: 0.688365\n",
      "Cost after iteration 2190: 0.688366\n",
      "Cost after iteration 2191: 0.688366\n",
      "Cost after iteration 2192: 0.688367\n",
      "Cost after iteration 2193: 0.688368\n",
      "Cost after iteration 2194: 0.688369\n",
      "Cost after iteration 2195: 0.688370\n",
      "Cost after iteration 2196: 0.688371\n",
      "Cost after iteration 2197: 0.688371\n",
      "Cost after iteration 2198: 0.688372\n",
      "Cost after iteration 2199: 0.688373\n",
      "Cost after iteration 2200: 0.688374\n",
      "Cost after iteration 2201: 0.688375\n",
      "Cost after iteration 2202: 0.688376\n",
      "Cost after iteration 2203: 0.688376\n",
      "Cost after iteration 2204: 0.688377\n",
      "Cost after iteration 2205: 0.688378\n",
      "Cost after iteration 2206: 0.688379\n",
      "Cost after iteration 2207: 0.688380\n",
      "Cost after iteration 2208: 0.688381\n",
      "Cost after iteration 2209: 0.688381\n",
      "Cost after iteration 2210: 0.688382\n",
      "Cost after iteration 2211: 0.688383\n",
      "Cost after iteration 2212: 0.688384\n",
      "Cost after iteration 2213: 0.688385\n",
      "Cost after iteration 2214: 0.688386\n",
      "Cost after iteration 2215: 0.688386\n",
      "Cost after iteration 2216: 0.688387\n",
      "Cost after iteration 2217: 0.688388\n",
      "Cost after iteration 2218: 0.688389\n",
      "Cost after iteration 2219: 0.688390\n",
      "Cost after iteration 2220: 0.688391\n",
      "Cost after iteration 2221: 0.688391\n",
      "Cost after iteration 2222: 0.688392\n",
      "Cost after iteration 2223: 0.688393\n",
      "Cost after iteration 2224: 0.688394\n",
      "Cost after iteration 2225: 0.688395\n",
      "Cost after iteration 2226: 0.688395\n",
      "Cost after iteration 2227: 0.688396\n",
      "Cost after iteration 2228: 0.688397\n",
      "Cost after iteration 2229: 0.688398\n",
      "Cost after iteration 2230: 0.688399\n",
      "Cost after iteration 2231: 0.688399\n",
      "Cost after iteration 2232: 0.688400\n",
      "Cost after iteration 2233: 0.688401\n",
      "Cost after iteration 2234: 0.688402\n",
      "Cost after iteration 2235: 0.688403\n",
      "Cost after iteration 2236: 0.688403\n",
      "Cost after iteration 2237: 0.688404\n",
      "Cost after iteration 2238: 0.688405\n",
      "Cost after iteration 2239: 0.688406\n",
      "Cost after iteration 2240: 0.688407\n",
      "Cost after iteration 2241: 0.688408\n",
      "Cost after iteration 2242: 0.688408\n",
      "Cost after iteration 2243: 0.688409\n",
      "Cost after iteration 2244: 0.688410\n",
      "Cost after iteration 2245: 0.688411\n",
      "Cost after iteration 2246: 0.688411\n",
      "Cost after iteration 2247: 0.688412\n",
      "Cost after iteration 2248: 0.688413\n",
      "Cost after iteration 2249: 0.688414\n",
      "Cost after iteration 2250: 0.688415\n",
      "Cost after iteration 2251: 0.688415\n",
      "Cost after iteration 2252: 0.688416\n",
      "Cost after iteration 2253: 0.688417\n",
      "Cost after iteration 2254: 0.688418\n",
      "Cost after iteration 2255: 0.688419\n",
      "Cost after iteration 2256: 0.688419\n",
      "Cost after iteration 2257: 0.688420\n",
      "Cost after iteration 2258: 0.688421\n",
      "Cost after iteration 2259: 0.688422\n",
      "Cost after iteration 2260: 0.688423\n",
      "Cost after iteration 2261: 0.688423\n",
      "Cost after iteration 2262: 0.688424\n",
      "Cost after iteration 2263: 0.688425\n",
      "Cost after iteration 2264: 0.688426\n",
      "Cost after iteration 2265: 0.688426\n",
      "Cost after iteration 2266: 0.688427\n",
      "Cost after iteration 2267: 0.688428\n",
      "Cost after iteration 2268: 0.688429\n",
      "Cost after iteration 2269: 0.688430\n",
      "Cost after iteration 2270: 0.688430\n",
      "Cost after iteration 2271: 0.688431\n",
      "Cost after iteration 2272: 0.688432\n",
      "Cost after iteration 2273: 0.688433\n",
      "Cost after iteration 2274: 0.688433\n",
      "Cost after iteration 2275: 0.688434\n",
      "Cost after iteration 2276: 0.688435\n",
      "Cost after iteration 2277: 0.688436\n",
      "Cost after iteration 2278: 0.688436\n",
      "Cost after iteration 2279: 0.688437\n",
      "Cost after iteration 2280: 0.688438\n",
      "Cost after iteration 2281: 0.688439\n",
      "Cost after iteration 2282: 0.688440\n",
      "Cost after iteration 2283: 0.688440\n",
      "Cost after iteration 2284: 0.688441\n",
      "Cost after iteration 2285: 0.688442\n",
      "Cost after iteration 2286: 0.688443\n",
      "Cost after iteration 2287: 0.688443\n",
      "Cost after iteration 2288: 0.688444\n",
      "Cost after iteration 2289: 0.688445\n",
      "Cost after iteration 2290: 0.688446\n",
      "Cost after iteration 2291: 0.688446\n",
      "Cost after iteration 2292: 0.688447\n",
      "Cost after iteration 2293: 0.688448\n",
      "Cost after iteration 2294: 0.688449\n",
      "Cost after iteration 2295: 0.688449\n",
      "Cost after iteration 2296: 0.688450\n",
      "Cost after iteration 2297: 0.688451\n",
      "Cost after iteration 2298: 0.688452\n",
      "Cost after iteration 2299: 0.688452\n",
      "Cost after iteration 2300: 0.688453\n",
      "Cost after iteration 2301: 0.688454\n",
      "Cost after iteration 2302: 0.688455\n",
      "Cost after iteration 2303: 0.688455\n",
      "Cost after iteration 2304: 0.688456\n",
      "Cost after iteration 2305: 0.688457\n",
      "Cost after iteration 2306: 0.688458\n",
      "Cost after iteration 2307: 0.688458\n",
      "Cost after iteration 2308: 0.688459\n",
      "Cost after iteration 2309: 0.688460\n",
      "Cost after iteration 2310: 0.688461\n",
      "Cost after iteration 2311: 0.688461\n",
      "Cost after iteration 2312: 0.688462\n",
      "Cost after iteration 2313: 0.688463\n",
      "Cost after iteration 2314: 0.688464\n",
      "Cost after iteration 2315: 0.688464\n",
      "Cost after iteration 2316: 0.688465\n",
      "Cost after iteration 2317: 0.688466\n",
      "Cost after iteration 2318: 0.688467\n",
      "Cost after iteration 2319: 0.688467\n",
      "Cost after iteration 2320: 0.688468\n",
      "Cost after iteration 2321: 0.688469\n",
      "Cost after iteration 2322: 0.688470\n",
      "Cost after iteration 2323: 0.688470\n",
      "Cost after iteration 2324: 0.688471\n",
      "Cost after iteration 2325: 0.688472\n",
      "Cost after iteration 2326: 0.688472\n",
      "Cost after iteration 2327: 0.688473\n",
      "Cost after iteration 2328: 0.688474\n",
      "Cost after iteration 2329: 0.688475\n",
      "Cost after iteration 2330: 0.688475\n",
      "Cost after iteration 2331: 0.688476\n",
      "Cost after iteration 2332: 0.688477\n",
      "Cost after iteration 2333: 0.688478\n",
      "Cost after iteration 2334: 0.688478\n",
      "Cost after iteration 2335: 0.688479\n",
      "Cost after iteration 2336: 0.688480\n",
      "Cost after iteration 2337: 0.688480\n",
      "Cost after iteration 2338: 0.688481\n",
      "Cost after iteration 2339: 0.688482\n",
      "Cost after iteration 2340: 0.688483\n",
      "Cost after iteration 2341: 0.688483\n",
      "Cost after iteration 2342: 0.688484\n",
      "Cost after iteration 2343: 0.688485\n",
      "Cost after iteration 2344: 0.688486\n",
      "Cost after iteration 2345: 0.688486\n",
      "Cost after iteration 2346: 0.688487\n",
      "Cost after iteration 2347: 0.688488\n",
      "Cost after iteration 2348: 0.688488\n",
      "Cost after iteration 2349: 0.688489\n",
      "Cost after iteration 2350: 0.688490\n",
      "Cost after iteration 2351: 0.688491\n",
      "Cost after iteration 2352: 0.688491\n",
      "Cost after iteration 2353: 0.688492\n",
      "Cost after iteration 2354: 0.688493\n",
      "Cost after iteration 2355: 0.688493\n",
      "Cost after iteration 2356: 0.688494\n",
      "Cost after iteration 2357: 0.688495\n",
      "Cost after iteration 2358: 0.688496\n",
      "Cost after iteration 2359: 0.688496\n",
      "Cost after iteration 2360: 0.688497\n",
      "Cost after iteration 2361: 0.688498\n",
      "Cost after iteration 2362: 0.688499\n",
      "Cost after iteration 2363: 0.688499\n",
      "Cost after iteration 2364: 0.688500\n",
      "Cost after iteration 2365: 0.688501\n",
      "Cost after iteration 2366: 0.688501\n",
      "Cost after iteration 2367: 0.688502\n",
      "Cost after iteration 2368: 0.688503\n",
      "Cost after iteration 2369: 0.688503\n",
      "Cost after iteration 2370: 0.688504\n",
      "Cost after iteration 2371: 0.688505\n",
      "Cost after iteration 2372: 0.688506\n",
      "Cost after iteration 2373: 0.688506\n",
      "Cost after iteration 2374: 0.688507\n",
      "Cost after iteration 2375: 0.688508\n",
      "Cost after iteration 2376: 0.688508\n",
      "Cost after iteration 2377: 0.688509\n",
      "Cost after iteration 2378: 0.688510\n",
      "Cost after iteration 2379: 0.688511\n",
      "Cost after iteration 2380: 0.688511\n",
      "Cost after iteration 2381: 0.688512\n",
      "Cost after iteration 2382: 0.688513\n",
      "Cost after iteration 2383: 0.688513\n",
      "Cost after iteration 2384: 0.688514\n",
      "Cost after iteration 2385: 0.688515\n",
      "Cost after iteration 2386: 0.688516\n",
      "Cost after iteration 2387: 0.688516\n",
      "Cost after iteration 2388: 0.688517\n",
      "Cost after iteration 2389: 0.688518\n",
      "Cost after iteration 2390: 0.688518\n",
      "Cost after iteration 2391: 0.688519\n",
      "Cost after iteration 2392: 0.688520\n",
      "Cost after iteration 2393: 0.688520\n",
      "Cost after iteration 2394: 0.688521\n",
      "Cost after iteration 2395: 0.688522\n",
      "Cost after iteration 2396: 0.688523\n",
      "Cost after iteration 2397: 0.688523\n",
      "Cost after iteration 2398: 0.688524\n",
      "Cost after iteration 2399: 0.688525\n",
      "Cost after iteration 2400: 0.688525\n",
      "Cost after iteration 2401: 0.688526\n",
      "Cost after iteration 2402: 0.688527\n",
      "Cost after iteration 2403: 0.688527\n",
      "Cost after iteration 2404: 0.688528\n",
      "Cost after iteration 2405: 0.688529\n",
      "Cost after iteration 2406: 0.688530\n",
      "Cost after iteration 2407: 0.688530\n",
      "Cost after iteration 2408: 0.688531\n",
      "Cost after iteration 2409: 0.688532\n",
      "Cost after iteration 2410: 0.688532\n",
      "Cost after iteration 2411: 0.688533\n",
      "Cost after iteration 2412: 0.688534\n",
      "Cost after iteration 2413: 0.688534\n",
      "Cost after iteration 2414: 0.688535\n",
      "Cost after iteration 2415: 0.688536\n",
      "Cost after iteration 2416: 0.688537\n",
      "Cost after iteration 2417: 0.688537\n",
      "Cost after iteration 2418: 0.688538\n",
      "Cost after iteration 2419: 0.688539\n",
      "Cost after iteration 2420: 0.688539\n",
      "Cost after iteration 2421: 0.688540\n",
      "Cost after iteration 2422: 0.688541\n",
      "Cost after iteration 2423: 0.688541\n",
      "Cost after iteration 2424: 0.688542\n",
      "Cost after iteration 2425: 0.688543\n",
      "Cost after iteration 2426: 0.688543\n",
      "Cost after iteration 2427: 0.688544\n",
      "Cost after iteration 2428: 0.688545\n",
      "Cost after iteration 2429: 0.688545\n",
      "Cost after iteration 2430: 0.688546\n",
      "Cost after iteration 2431: 0.688547\n",
      "Cost after iteration 2432: 0.688548\n",
      "Cost after iteration 2433: 0.688548\n",
      "Cost after iteration 2434: 0.688549\n",
      "Cost after iteration 2435: 0.688550\n",
      "Cost after iteration 2436: 0.688550\n",
      "Cost after iteration 2437: 0.688551\n",
      "Cost after iteration 2438: 0.688552\n",
      "Cost after iteration 2439: 0.688552\n",
      "Cost after iteration 2440: 0.688553\n",
      "Cost after iteration 2441: 0.688554\n",
      "Cost after iteration 2442: 0.688554\n",
      "Cost after iteration 2443: 0.688555\n",
      "Cost after iteration 2444: 0.688556\n",
      "Cost after iteration 2445: 0.688556\n",
      "Cost after iteration 2446: 0.688557\n",
      "Cost after iteration 2447: 0.688558\n",
      "Cost after iteration 2448: 0.688559\n",
      "Cost after iteration 2449: 0.688559\n",
      "Cost after iteration 2450: 0.688560\n",
      "Cost after iteration 2451: 0.688561\n",
      "Cost after iteration 2452: 0.688561\n",
      "Cost after iteration 2453: 0.688562\n",
      "Cost after iteration 2454: 0.688563\n",
      "Cost after iteration 2455: 0.688563\n",
      "Cost after iteration 2456: 0.688564\n",
      "Cost after iteration 2457: 0.688565\n",
      "Cost after iteration 2458: 0.688565\n",
      "Cost after iteration 2459: 0.688566\n",
      "Cost after iteration 2460: 0.688567\n",
      "Cost after iteration 2461: 0.688567\n",
      "Cost after iteration 2462: 0.688568\n",
      "Cost after iteration 2463: 0.688569\n",
      "Cost after iteration 2464: 0.688569\n",
      "Cost after iteration 2465: 0.688570\n",
      "Cost after iteration 2466: 0.688571\n",
      "Cost after iteration 2467: 0.688571\n",
      "Cost after iteration 2468: 0.688572\n",
      "Cost after iteration 2469: 0.688573\n",
      "Cost after iteration 2470: 0.688574\n",
      "Cost after iteration 2471: 0.688574\n",
      "Cost after iteration 2472: 0.688575\n",
      "Cost after iteration 2473: 0.688576\n",
      "Cost after iteration 2474: 0.688576\n",
      "Cost after iteration 2475: 0.688577\n",
      "Cost after iteration 2476: 0.688578\n",
      "Cost after iteration 2477: 0.688578\n",
      "Cost after iteration 2478: 0.688579\n",
      "Cost after iteration 2479: 0.688580\n",
      "Cost after iteration 2480: 0.688580\n",
      "Cost after iteration 2481: 0.688581\n",
      "Cost after iteration 2482: 0.688582\n",
      "Cost after iteration 2483: 0.688582\n",
      "Cost after iteration 2484: 0.688583\n",
      "Cost after iteration 2485: 0.688584\n",
      "Cost after iteration 2486: 0.688584\n",
      "Cost after iteration 2487: 0.688585\n",
      "Cost after iteration 2488: 0.688586\n",
      "Cost after iteration 2489: 0.688586\n",
      "Cost after iteration 2490: 0.688587\n",
      "Cost after iteration 2491: 0.688588\n",
      "Cost after iteration 2492: 0.688588\n",
      "Cost after iteration 2493: 0.688589\n",
      "Cost after iteration 2494: 0.688590\n",
      "Cost after iteration 2495: 0.688590\n",
      "Cost after iteration 2496: 0.688591\n",
      "Cost after iteration 2497: 0.688592\n",
      "Cost after iteration 2498: 0.688592\n",
      "Cost after iteration 2499: 0.688593\n",
      "Cost after iteration 2500: 0.688594\n",
      "Cost after iteration 2501: 0.688594\n",
      "Cost after iteration 2502: 0.688595\n",
      "Cost after iteration 2503: 0.688596\n",
      "Cost after iteration 2504: 0.688596\n",
      "Cost after iteration 2505: 0.688597\n",
      "Cost after iteration 2506: 0.688598\n",
      "Cost after iteration 2507: 0.688598\n",
      "Cost after iteration 2508: 0.688599\n",
      "Cost after iteration 2509: 0.688600\n",
      "Cost after iteration 2510: 0.688600\n",
      "Cost after iteration 2511: 0.688601\n",
      "Cost after iteration 2512: 0.688602\n",
      "Cost after iteration 2513: 0.688602\n",
      "Cost after iteration 2514: 0.688603\n",
      "Cost after iteration 2515: 0.688604\n",
      "Cost after iteration 2516: 0.688604\n",
      "Cost after iteration 2517: 0.688605\n",
      "Cost after iteration 2518: 0.688606\n",
      "Cost after iteration 2519: 0.688606\n",
      "Cost after iteration 2520: 0.688607\n",
      "Cost after iteration 2521: 0.688608\n",
      "Cost after iteration 2522: 0.688608\n",
      "Cost after iteration 2523: 0.688609\n",
      "Cost after iteration 2524: 0.688610\n",
      "Cost after iteration 2525: 0.688610\n",
      "Cost after iteration 2526: 0.688611\n",
      "Cost after iteration 2527: 0.688611\n",
      "Cost after iteration 2528: 0.688612\n",
      "Cost after iteration 2529: 0.688613\n",
      "Cost after iteration 2530: 0.688613\n",
      "Cost after iteration 2531: 0.688614\n",
      "Cost after iteration 2532: 0.688615\n",
      "Cost after iteration 2533: 0.688615\n",
      "Cost after iteration 2534: 0.688616\n",
      "Cost after iteration 2535: 0.688617\n",
      "Cost after iteration 2536: 0.688617\n",
      "Cost after iteration 2537: 0.688618\n",
      "Cost after iteration 2538: 0.688619\n",
      "Cost after iteration 2539: 0.688619\n",
      "Cost after iteration 2540: 0.688620\n",
      "Cost after iteration 2541: 0.688621\n",
      "Cost after iteration 2542: 0.688621\n",
      "Cost after iteration 2543: 0.688622\n",
      "Cost after iteration 2544: 0.688623\n",
      "Cost after iteration 2545: 0.688623\n",
      "Cost after iteration 2546: 0.688624\n",
      "Cost after iteration 2547: 0.688624\n",
      "Cost after iteration 2548: 0.688625\n",
      "Cost after iteration 2549: 0.688626\n",
      "Cost after iteration 2550: 0.688626\n",
      "Cost after iteration 2551: 0.688627\n",
      "Cost after iteration 2552: 0.688628\n",
      "Cost after iteration 2553: 0.688628\n",
      "Cost after iteration 2554: 0.688629\n",
      "Cost after iteration 2555: 0.688630\n",
      "Cost after iteration 2556: 0.688630\n",
      "Cost after iteration 2557: 0.688631\n",
      "Cost after iteration 2558: 0.688631\n",
      "Cost after iteration 2559: 0.688632\n",
      "Cost after iteration 2560: 0.688633\n",
      "Cost after iteration 2561: 0.688633\n",
      "Cost after iteration 2562: 0.688634\n",
      "Cost after iteration 2563: 0.688635\n",
      "Cost after iteration 2564: 0.688635\n",
      "Cost after iteration 2565: 0.688636\n",
      "Cost after iteration 2566: 0.688637\n",
      "Cost after iteration 2567: 0.688637\n",
      "Cost after iteration 2568: 0.688638\n",
      "Cost after iteration 2569: 0.688638\n",
      "Cost after iteration 2570: 0.688639\n",
      "Cost after iteration 2571: 0.688640\n",
      "Cost after iteration 2572: 0.688640\n",
      "Cost after iteration 2573: 0.688641\n",
      "Cost after iteration 2574: 0.688642\n",
      "Cost after iteration 2575: 0.688642\n",
      "Cost after iteration 2576: 0.688643\n",
      "Cost after iteration 2577: 0.688643\n",
      "Cost after iteration 2578: 0.688644\n",
      "Cost after iteration 2579: 0.688645\n",
      "Cost after iteration 2580: 0.688645\n",
      "Cost after iteration 2581: 0.688646\n",
      "Cost after iteration 2582: 0.688646\n",
      "Cost after iteration 2583: 0.688647\n",
      "Cost after iteration 2584: 0.688648\n",
      "Cost after iteration 2585: 0.688648\n",
      "Cost after iteration 2586: 0.688649\n",
      "Cost after iteration 2587: 0.688650\n",
      "Cost after iteration 2588: 0.688650\n",
      "Cost after iteration 2589: 0.688651\n",
      "Cost after iteration 2590: 0.688651\n",
      "Cost after iteration 2591: 0.688652\n",
      "Cost after iteration 2592: 0.688653\n",
      "Cost after iteration 2593: 0.688653\n",
      "Cost after iteration 2594: 0.688654\n",
      "Cost after iteration 2595: 0.688654\n",
      "Cost after iteration 2596: 0.688655\n",
      "Cost after iteration 2597: 0.688656\n",
      "Cost after iteration 2598: 0.688656\n",
      "Cost after iteration 2599: 0.688657\n",
      "Cost after iteration 2600: 0.688657\n",
      "Cost after iteration 2601: 0.688658\n",
      "Cost after iteration 2602: 0.688659\n",
      "Cost after iteration 2603: 0.688659\n",
      "Cost after iteration 2604: 0.688660\n",
      "Cost after iteration 2605: 0.688660\n",
      "Cost after iteration 2606: 0.688661\n",
      "Cost after iteration 2607: 0.688662\n",
      "Cost after iteration 2608: 0.688662\n",
      "Cost after iteration 2609: 0.688663\n",
      "Cost after iteration 2610: 0.688663\n",
      "Cost after iteration 2611: 0.688664\n",
      "Cost after iteration 2612: 0.688665\n",
      "Cost after iteration 2613: 0.688665\n",
      "Cost after iteration 2614: 0.688666\n",
      "Cost after iteration 2615: 0.688666\n",
      "Cost after iteration 2616: 0.688667\n",
      "Cost after iteration 2617: 0.688668\n",
      "Cost after iteration 2618: 0.688668\n",
      "Cost after iteration 2619: 0.688669\n",
      "Cost after iteration 2620: 0.688669\n",
      "Cost after iteration 2621: 0.688670\n",
      "Cost after iteration 2622: 0.688670\n",
      "Cost after iteration 2623: 0.688671\n",
      "Cost after iteration 2624: 0.688672\n",
      "Cost after iteration 2625: 0.688672\n",
      "Cost after iteration 2626: 0.688673\n",
      "Cost after iteration 2627: 0.688673\n",
      "Cost after iteration 2628: 0.688674\n",
      "Cost after iteration 2629: 0.688675\n",
      "Cost after iteration 2630: 0.688675\n",
      "Cost after iteration 2631: 0.688676\n",
      "Cost after iteration 2632: 0.688676\n",
      "Cost after iteration 2633: 0.688677\n",
      "Cost after iteration 2634: 0.688677\n",
      "Cost after iteration 2635: 0.688678\n",
      "Cost after iteration 2636: 0.688679\n",
      "Cost after iteration 2637: 0.688679\n",
      "Cost after iteration 2638: 0.688680\n",
      "Cost after iteration 2639: 0.688680\n",
      "Cost after iteration 2640: 0.688681\n",
      "Cost after iteration 2641: 0.688681\n",
      "Cost after iteration 2642: 0.688682\n",
      "Cost after iteration 2643: 0.688682\n",
      "Cost after iteration 2644: 0.688683\n",
      "Cost after iteration 2645: 0.688684\n",
      "Cost after iteration 2646: 0.688684\n",
      "Cost after iteration 2647: 0.688685\n",
      "Cost after iteration 2648: 0.688685\n",
      "Cost after iteration 2649: 0.688686\n",
      "Cost after iteration 2650: 0.688686\n",
      "Cost after iteration 2651: 0.688687\n",
      "Cost after iteration 2652: 0.688688\n",
      "Cost after iteration 2653: 0.688688\n",
      "Cost after iteration 2654: 0.688689\n",
      "Cost after iteration 2655: 0.688689\n",
      "Cost after iteration 2656: 0.688690\n",
      "Cost after iteration 2657: 0.688690\n",
      "Cost after iteration 2658: 0.688691\n",
      "Cost after iteration 2659: 0.688691\n",
      "Cost after iteration 2660: 0.688692\n",
      "Cost after iteration 2661: 0.688692\n",
      "Cost after iteration 2662: 0.688693\n",
      "Cost after iteration 2663: 0.688694\n",
      "Cost after iteration 2664: 0.688694\n",
      "Cost after iteration 2665: 0.688695\n",
      "Cost after iteration 2666: 0.688695\n",
      "Cost after iteration 2667: 0.688696\n",
      "Cost after iteration 2668: 0.688696\n",
      "Cost after iteration 2669: 0.688697\n",
      "Cost after iteration 2670: 0.688697\n",
      "Cost after iteration 2671: 0.688698\n",
      "Cost after iteration 2672: 0.688698\n",
      "Cost after iteration 2673: 0.688699\n",
      "Cost after iteration 2674: 0.688700\n",
      "Cost after iteration 2675: 0.688700\n",
      "Cost after iteration 2676: 0.688701\n",
      "Cost after iteration 2677: 0.688701\n",
      "Cost after iteration 2678: 0.688702\n",
      "Cost after iteration 2679: 0.688702\n",
      "Cost after iteration 2680: 0.688703\n",
      "Cost after iteration 2681: 0.688703\n",
      "Cost after iteration 2682: 0.688704\n",
      "Cost after iteration 2683: 0.688704\n",
      "Cost after iteration 2684: 0.688705\n",
      "Cost after iteration 2685: 0.688705\n",
      "Cost after iteration 2686: 0.688706\n",
      "Cost after iteration 2687: 0.688706\n",
      "Cost after iteration 2688: 0.688707\n",
      "Cost after iteration 2689: 0.688707\n",
      "Cost after iteration 2690: 0.688708\n",
      "Cost after iteration 2691: 0.688708\n",
      "Cost after iteration 2692: 0.688709\n",
      "Cost after iteration 2693: 0.688710\n",
      "Cost after iteration 2694: 0.688710\n",
      "Cost after iteration 2695: 0.688711\n",
      "Cost after iteration 2696: 0.688711\n",
      "Cost after iteration 2697: 0.688712\n",
      "Cost after iteration 2698: 0.688712\n",
      "Cost after iteration 2699: 0.688713\n",
      "Cost after iteration 2700: 0.688713\n",
      "Cost after iteration 2701: 0.688714\n",
      "Cost after iteration 2702: 0.688714\n",
      "Cost after iteration 2703: 0.688715\n",
      "Cost after iteration 2704: 0.688715\n",
      "Cost after iteration 2705: 0.688716\n",
      "Cost after iteration 2706: 0.688716\n",
      "Cost after iteration 2707: 0.688717\n",
      "Cost after iteration 2708: 0.688717\n",
      "Cost after iteration 2709: 0.688718\n",
      "Cost after iteration 2710: 0.688718\n",
      "Cost after iteration 2711: 0.688719\n",
      "Cost after iteration 2712: 0.688719\n",
      "Cost after iteration 2713: 0.688720\n",
      "Cost after iteration 2714: 0.688720\n",
      "Cost after iteration 2715: 0.688721\n",
      "Cost after iteration 2716: 0.688721\n",
      "Cost after iteration 2717: 0.688722\n",
      "Cost after iteration 2718: 0.688722\n",
      "Cost after iteration 2719: 0.688723\n",
      "Cost after iteration 2720: 0.688723\n",
      "Cost after iteration 2721: 0.688724\n",
      "Cost after iteration 2722: 0.688724\n",
      "Cost after iteration 2723: 0.688725\n",
      "Cost after iteration 2724: 0.688725\n",
      "Cost after iteration 2725: 0.688726\n",
      "Cost after iteration 2726: 0.688726\n",
      "Cost after iteration 2727: 0.688727\n",
      "Cost after iteration 2728: 0.688727\n",
      "Cost after iteration 2729: 0.688728\n",
      "Cost after iteration 2730: 0.688728\n",
      "Cost after iteration 2731: 0.688729\n",
      "Cost after iteration 2732: 0.688729\n",
      "Cost after iteration 2733: 0.688730\n",
      "Cost after iteration 2734: 0.688730\n",
      "Cost after iteration 2735: 0.688731\n",
      "Cost after iteration 2736: 0.688731\n",
      "Cost after iteration 2737: 0.688731\n",
      "Cost after iteration 2738: 0.688732\n",
      "Cost after iteration 2739: 0.688732\n",
      "Cost after iteration 2740: 0.688733\n",
      "Cost after iteration 2741: 0.688733\n",
      "Cost after iteration 2742: 0.688734\n",
      "Cost after iteration 2743: 0.688734\n",
      "Cost after iteration 2744: 0.688735\n",
      "Cost after iteration 2745: 0.688735\n",
      "Cost after iteration 2746: 0.688736\n",
      "Cost after iteration 2747: 0.688736\n",
      "Cost after iteration 2748: 0.688737\n",
      "Cost after iteration 2749: 0.688737\n",
      "Cost after iteration 2750: 0.688738\n",
      "Cost after iteration 2751: 0.688738\n",
      "Cost after iteration 2752: 0.688739\n",
      "Cost after iteration 2753: 0.688739\n",
      "Cost after iteration 2754: 0.688740\n",
      "Cost after iteration 2755: 0.688740\n",
      "Cost after iteration 2756: 0.688740\n",
      "Cost after iteration 2757: 0.688741\n",
      "Cost after iteration 2758: 0.688741\n",
      "Cost after iteration 2759: 0.688742\n",
      "Cost after iteration 2760: 0.688742\n",
      "Cost after iteration 2761: 0.688743\n",
      "Cost after iteration 2762: 0.688743\n",
      "Cost after iteration 2763: 0.688744\n",
      "Cost after iteration 2764: 0.688744\n",
      "Cost after iteration 2765: 0.688745\n",
      "Cost after iteration 2766: 0.688745\n",
      "Cost after iteration 2767: 0.688746\n",
      "Cost after iteration 2768: 0.688746\n",
      "Cost after iteration 2769: 0.688746\n",
      "Cost after iteration 2770: 0.688747\n",
      "Cost after iteration 2771: 0.688747\n",
      "Cost after iteration 2772: 0.688748\n",
      "Cost after iteration 2773: 0.688748\n",
      "Cost after iteration 2774: 0.688749\n",
      "Cost after iteration 2775: 0.688749\n",
      "Cost after iteration 2776: 0.688750\n",
      "Cost after iteration 2777: 0.688750\n",
      "Cost after iteration 2778: 0.688751\n",
      "Cost after iteration 2779: 0.688751\n",
      "Cost after iteration 2780: 0.688751\n",
      "Cost after iteration 2781: 0.688752\n",
      "Cost after iteration 2782: 0.688752\n",
      "Cost after iteration 2783: 0.688753\n",
      "Cost after iteration 2784: 0.688753\n",
      "Cost after iteration 2785: 0.688754\n",
      "Cost after iteration 2786: 0.688754\n",
      "Cost after iteration 2787: 0.688755\n",
      "Cost after iteration 2788: 0.688755\n",
      "Cost after iteration 2789: 0.688756\n",
      "Cost after iteration 2790: 0.688756\n",
      "Cost after iteration 2791: 0.688756\n",
      "Cost after iteration 2792: 0.688757\n",
      "Cost after iteration 2793: 0.688757\n",
      "Cost after iteration 2794: 0.688758\n",
      "Cost after iteration 2795: 0.688758\n",
      "Cost after iteration 2796: 0.688759\n",
      "Cost after iteration 2797: 0.688759\n",
      "Cost after iteration 2798: 0.688760\n",
      "Cost after iteration 2799: 0.688760\n",
      "Cost after iteration 2800: 0.688760\n",
      "Cost after iteration 2801: 0.688761\n",
      "Cost after iteration 2802: 0.688761\n",
      "Cost after iteration 2803: 0.688762\n",
      "Cost after iteration 2804: 0.688762\n",
      "Cost after iteration 2805: 0.688763\n",
      "Cost after iteration 2806: 0.688763\n",
      "Cost after iteration 2807: 0.688763\n",
      "Cost after iteration 2808: 0.688764\n",
      "Cost after iteration 2809: 0.688764\n",
      "Cost after iteration 2810: 0.688765\n",
      "Cost after iteration 2811: 0.688765\n",
      "Cost after iteration 2812: 0.688766\n",
      "Cost after iteration 2813: 0.688766\n",
      "Cost after iteration 2814: 0.688766\n",
      "Cost after iteration 2815: 0.688767\n",
      "Cost after iteration 2816: 0.688767\n",
      "Cost after iteration 2817: 0.688768\n",
      "Cost after iteration 2818: 0.688768\n",
      "Cost after iteration 2819: 0.688769\n",
      "Cost after iteration 2820: 0.688769\n",
      "Cost after iteration 2821: 0.688769\n",
      "Cost after iteration 2822: 0.688770\n",
      "Cost after iteration 2823: 0.688770\n",
      "Cost after iteration 2824: 0.688771\n",
      "Cost after iteration 2825: 0.688771\n",
      "Cost after iteration 2826: 0.688772\n",
      "Cost after iteration 2827: 0.688772\n",
      "Cost after iteration 2828: 0.688772\n",
      "Cost after iteration 2829: 0.688773\n",
      "Cost after iteration 2830: 0.688773\n",
      "Cost after iteration 2831: 0.688774\n",
      "Cost after iteration 2832: 0.688774\n",
      "Cost after iteration 2833: 0.688775\n",
      "Cost after iteration 2834: 0.688775\n",
      "Cost after iteration 2835: 0.688775\n",
      "Cost after iteration 2836: 0.688776\n",
      "Cost after iteration 2837: 0.688776\n",
      "Cost after iteration 2838: 0.688777\n",
      "Cost after iteration 2839: 0.688777\n",
      "Cost after iteration 2840: 0.688778\n",
      "Cost after iteration 2841: 0.688778\n",
      "Cost after iteration 2842: 0.688778\n",
      "Cost after iteration 2843: 0.688779\n",
      "Cost after iteration 2844: 0.688779\n",
      "Cost after iteration 2845: 0.688780\n",
      "Cost after iteration 2846: 0.688780\n",
      "Cost after iteration 2847: 0.688780\n",
      "Cost after iteration 2848: 0.688781\n",
      "Cost after iteration 2849: 0.688781\n",
      "Cost after iteration 2850: 0.688782\n",
      "Cost after iteration 2851: 0.688782\n",
      "Cost after iteration 2852: 0.688783\n",
      "Cost after iteration 2853: 0.688783\n",
      "Cost after iteration 2854: 0.688783\n",
      "Cost after iteration 2855: 0.688784\n",
      "Cost after iteration 2856: 0.688784\n",
      "Cost after iteration 2857: 0.688785\n",
      "Cost after iteration 2858: 0.688785\n",
      "Cost after iteration 2859: 0.688785\n",
      "Cost after iteration 2860: 0.688786\n",
      "Cost after iteration 2861: 0.688786\n",
      "Cost after iteration 2862: 0.688787\n",
      "Cost after iteration 2863: 0.688787\n",
      "Cost after iteration 2864: 0.688787\n",
      "Cost after iteration 2865: 0.688788\n",
      "Cost after iteration 2866: 0.688788\n",
      "Cost after iteration 2867: 0.688789\n",
      "Cost after iteration 2868: 0.688789\n",
      "Cost after iteration 2869: 0.688789\n",
      "Cost after iteration 2870: 0.688790\n",
      "Cost after iteration 2871: 0.688790\n",
      "Cost after iteration 2872: 0.688791\n",
      "Cost after iteration 2873: 0.688791\n",
      "Cost after iteration 2874: 0.688792\n",
      "Cost after iteration 2875: 0.688792\n",
      "Cost after iteration 2876: 0.688792\n",
      "Cost after iteration 2877: 0.688793\n",
      "Cost after iteration 2878: 0.688793\n",
      "Cost after iteration 2879: 0.688794\n",
      "Cost after iteration 2880: 0.688794\n",
      "Cost after iteration 2881: 0.688794\n",
      "Cost after iteration 2882: 0.688795\n",
      "Cost after iteration 2883: 0.688795\n",
      "Cost after iteration 2884: 0.688796\n",
      "Cost after iteration 2885: 0.688796\n",
      "Cost after iteration 2886: 0.688796\n",
      "Cost after iteration 2887: 0.688797\n",
      "Cost after iteration 2888: 0.688797\n",
      "Cost after iteration 2889: 0.688798\n",
      "Cost after iteration 2890: 0.688798\n",
      "Cost after iteration 2891: 0.688798\n",
      "Cost after iteration 2892: 0.688799\n",
      "Cost after iteration 2893: 0.688799\n",
      "Cost after iteration 2894: 0.688800\n",
      "Cost after iteration 2895: 0.688800\n",
      "Cost after iteration 2896: 0.688800\n",
      "Cost after iteration 2897: 0.688801\n",
      "Cost after iteration 2898: 0.688801\n",
      "Cost after iteration 2899: 0.688802\n",
      "Cost after iteration 2900: 0.688802\n",
      "Cost after iteration 2901: 0.688802\n",
      "Cost after iteration 2902: 0.688803\n",
      "Cost after iteration 2903: 0.688803\n",
      "Cost after iteration 2904: 0.688803\n",
      "Cost after iteration 2905: 0.688804\n",
      "Cost after iteration 2906: 0.688804\n",
      "Cost after iteration 2907: 0.688805\n",
      "Cost after iteration 2908: 0.688805\n",
      "Cost after iteration 2909: 0.688805\n",
      "Cost after iteration 2910: 0.688806\n",
      "Cost after iteration 2911: 0.688806\n",
      "Cost after iteration 2912: 0.688807\n",
      "Cost after iteration 2913: 0.688807\n",
      "Cost after iteration 2914: 0.688807\n",
      "Cost after iteration 2915: 0.688808\n",
      "Cost after iteration 2916: 0.688808\n",
      "Cost after iteration 2917: 0.688809\n",
      "Cost after iteration 2918: 0.688809\n",
      "Cost after iteration 2919: 0.688809\n",
      "Cost after iteration 2920: 0.688810\n",
      "Cost after iteration 2921: 0.688810\n",
      "Cost after iteration 2922: 0.688811\n",
      "Cost after iteration 2923: 0.688811\n",
      "Cost after iteration 2924: 0.688811\n",
      "Cost after iteration 2925: 0.688812\n",
      "Cost after iteration 2926: 0.688812\n",
      "Cost after iteration 2927: 0.688812\n",
      "Cost after iteration 2928: 0.688813\n",
      "Cost after iteration 2929: 0.688813\n",
      "Cost after iteration 2930: 0.688814\n",
      "Cost after iteration 2931: 0.688814\n",
      "Cost after iteration 2932: 0.688814\n",
      "Cost after iteration 2933: 0.688815\n",
      "Cost after iteration 2934: 0.688815\n",
      "Cost after iteration 2935: 0.688816\n",
      "Cost after iteration 2936: 0.688816\n",
      "Cost after iteration 2937: 0.688816\n",
      "Cost after iteration 2938: 0.688817\n",
      "Cost after iteration 2939: 0.688817\n",
      "Cost after iteration 2940: 0.688818\n",
      "Cost after iteration 2941: 0.688818\n",
      "Cost after iteration 2942: 0.688818\n",
      "Cost after iteration 2943: 0.688819\n",
      "Cost after iteration 2944: 0.688819\n",
      "Cost after iteration 2945: 0.688819\n",
      "Cost after iteration 2946: 0.688820\n",
      "Cost after iteration 2947: 0.688820\n",
      "Cost after iteration 2948: 0.688821\n",
      "Cost after iteration 2949: 0.688821\n",
      "Cost after iteration 2950: 0.688821\n",
      "Cost after iteration 2951: 0.688822\n",
      "Cost after iteration 2952: 0.688822\n",
      "Cost after iteration 2953: 0.688823\n",
      "Cost after iteration 2954: 0.688823\n",
      "Cost after iteration 2955: 0.688823\n",
      "Cost after iteration 2956: 0.688824\n",
      "Cost after iteration 2957: 0.688824\n",
      "Cost after iteration 2958: 0.688824\n",
      "Cost after iteration 2959: 0.688825\n",
      "Cost after iteration 2960: 0.688825\n",
      "Cost after iteration 2961: 0.688826\n",
      "Cost after iteration 2962: 0.688826\n",
      "Cost after iteration 2963: 0.688826\n",
      "Cost after iteration 2964: 0.688827\n",
      "Cost after iteration 2965: 0.688827\n",
      "Cost after iteration 2966: 0.688827\n",
      "Cost after iteration 2967: 0.688828\n",
      "Cost after iteration 2968: 0.688828\n",
      "Cost after iteration 2969: 0.688829\n",
      "Cost after iteration 2970: 0.688829\n",
      "Cost after iteration 2971: 0.688829\n",
      "Cost after iteration 2972: 0.688830\n",
      "Cost after iteration 2973: 0.688830\n",
      "Cost after iteration 2974: 0.688831\n",
      "Cost after iteration 2975: 0.688831\n",
      "Cost after iteration 2976: 0.688831\n",
      "Cost after iteration 2977: 0.688832\n",
      "Cost after iteration 2978: 0.688832\n",
      "Cost after iteration 2979: 0.688832\n",
      "Cost after iteration 2980: 0.688833\n",
      "Cost after iteration 2981: 0.688833\n",
      "Cost after iteration 2982: 0.688834\n",
      "Cost after iteration 2983: 0.688834\n",
      "Cost after iteration 2984: 0.688834\n",
      "Cost after iteration 2985: 0.688835\n",
      "Cost after iteration 2986: 0.688835\n",
      "Cost after iteration 2987: 0.688835\n",
      "Cost after iteration 2988: 0.688836\n",
      "Cost after iteration 2989: 0.688836\n",
      "Cost after iteration 2990: 0.688837\n",
      "Cost after iteration 2991: 0.688837\n",
      "Cost after iteration 2992: 0.688837\n",
      "Cost after iteration 2993: 0.688838\n",
      "Cost after iteration 2994: 0.688838\n",
      "Cost after iteration 2995: 0.688838\n",
      "Cost after iteration 2996: 0.688839\n",
      "Cost after iteration 2997: 0.688839\n",
      "Cost after iteration 2998: 0.688840\n",
      "Cost after iteration 2999: 0.688840\n",
      "W1 = [[ 265.02879275 -239.61472806]\n",
      " [ 224.60314789 -203.16530761]]\n",
      "b1 = [[-1.71940101]\n",
      " [-0.87605312]]\n",
      "W2 = [[-0.09721954 -0.08982342]]\n",
      "b2 = [[-0.01773562]]\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(X, Y, n_h=2, num_iterations=3000, learning_rate=1.2, print_cost=True)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b2 = parameters[\"b2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "Note: the actual values can be different!\n",
    "\n",
    "```Python\n",
    "Cost after iteration 0: 0.693148\n",
    "Cost after iteration 1: 0.693147\n",
    "Cost after iteration 2: 0.693147\n",
    "Cost after iteration 3: 0.693147\n",
    "Cost after iteration 4: 0.693147\n",
    "Cost after iteration 5: 0.693147\n",
    "...\n",
    "Cost after iteration 2995: 0.209524\n",
    "Cost after iteration 2996: 0.208025\n",
    "Cost after iteration 2997: 0.210427\n",
    "Cost after iteration 2998: 0.208929\n",
    "Cost after iteration 2999: 0.211306\n",
    "W1 = [[ 2.14274251 -1.93155541]\n",
    " [ 2.20268789 -2.1131799 ]]\n",
    "b1 = [[-4.83079243]\n",
    " [ 6.2845223 ]]\n",
    "W2 = [[-7.21370685  7.0898022 ]]\n",
    "b2 = [[-3.48755239]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Note: \n",
    "# Actual values are not checked here in the unit tests (due to random initialization).\n",
    "w3_unittest.test_nn_model(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model parameters can be used to find the boundary line and for making predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex08'></a>\n",
    "### Exercise 8\n",
    "\n",
    "Computes probabilities using forward propagation, and make classification to 0/1 using 0.5 as the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (blue: 0 / red: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (в‰€ 2 lines of code)\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.round(A2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates (in the columns):\n",
      "[[2 8 2 8]\n",
      " [2 8 8 2]]\n",
      "Predictions:\n",
      "[[0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_pred = np.array([[2, 8, 2, 8], [2, 8, 8, 2]])\n",
    "Y_pred = predict(X_pred, parameters)\n",
    "\n",
    "print(f\"Coordinates (in the columns):\\n{X_pred}\")\n",
    "print(f\"Predictions:\\n{Y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ \n",
    "\n",
    "```Python\n",
    "Coordinates (in the columns):\n",
    "[[2 8 2 8]\n",
    " [2 8 8 2]]\n",
    "Predictions:\n",
    "[[ True  True False False]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w3_unittest.test_predict(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the boundary line. Do not worry if you don't understand the function `plot_decision_boundary` line by line - it simply makes prediction for some points on the plane and plots them as a contour plot (just two colors - blue and red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(predict, parameters, X, Y):\n",
    "    # Define bounds of the domain.\n",
    "    min1, max1 = X[0, :].min()-1, X[0, :].max()+1\n",
    "    min2, max2 = X[1, :].min()-1, X[1, :].max()+1\n",
    "    # Define the x and y scale.\n",
    "    x1grid = np.arange(min1, max1, 0.1)\n",
    "    x2grid = np.arange(min2, max2, 0.1)\n",
    "    # Create all of the lines and rows of the grid.\n",
    "    xx, yy = np.meshgrid(x1grid, x2grid)\n",
    "    # Flatten each grid to a vector.\n",
    "    r1, r2 = xx.flatten(), yy.flatten()\n",
    "    r1, r2 = r1.reshape((1, len(r1))), r2.reshape((1, len(r2)))\n",
    "    # Vertical stack vectors to create x1,x2 input for the model.\n",
    "    grid = np.vstack((r1,r2))\n",
    "    # Make predictions for the grid.\n",
    "    predictions = predict(grid, parameters)\n",
    "    # Reshape the predictions back into a grid.\n",
    "    zz = predictions.reshape(xx.shape)\n",
    "    # Plot the grid of x, y and z values as a surface.\n",
    "    plt.contourf(xx, yy, zz, cmap=plt.cm.Spectral.reversed())\n",
    "    plt.scatter(X[0, :], X[1, :], c=Y, cmap=colors.ListedColormap(['blue', 'red']));\n",
    "\n",
    "# Plot the decision boundary.\n",
    "plot_decision_boundary(predict, parameters, X, Y)\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(n_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great, you can see that more complicated classification problems can be solved with two layer neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Optional: Other Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a slightly different dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "samples, labels = make_blobs(n_samples=n_samples, \n",
    "                             centers=([2.5, 3], [6.7, 7.9], [2.1, 7.9], [7.4, 2.8]), \n",
    "                             cluster_std=1.1,\n",
    "                             random_state=0)\n",
    "labels[(labels == 0)] = 0\n",
    "labels[(labels == 1)] = 1\n",
    "labels[(labels == 2) | (labels == 3)] = 1\n",
    "X_2 = np.transpose(samples)\n",
    "Y_2 = labels.reshape((1,n_samples))\n",
    "\n",
    "plt.scatter(X_2[0, :], X_2[1, :], c=Y_2, cmap=colors.ListedColormap(['blue', 'red']));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when building your neural network, a number of the nodes in the hidden layer could be taken as a parameter. Try to change this parameter and investigate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters_2 = nn_model(X_2, Y_2, n_h=1, num_iterations=3000, learning_rate=1.2, print_cost=False)\n",
    "parameters_2 = nn_model(X_2, Y_2, n_h=2, num_iterations=3000, learning_rate=1.2, print_cost=False)\n",
    "# parameters_2 = nn_model(X_2, Y_2, n_h=15, num_iterations=3000, learning_rate=1.2, print_cost=False)\n",
    "\n",
    "# This function will call predict function \n",
    "plot_decision_boundary(predict, parameters_2, X_2, Y_2)\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(n_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are some misclassified points - real-world datasets are usually linearly inseparable, and there will be a small percentage of errors. More than that, you do not want to build a model that fits too closely, almost exactly to a particular set of data - it may fail to predict future observations. This problem is known as **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on finishing this programming assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C1_W1_Assignment_Solution.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "AI4MC1-1"
   ]
  },
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
